{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soft** Binary Classification:\n",
    "\n",
    "想知道發生(+1)的機率是多少，介於 0~1 之間。\n",
    "\n",
    "$ f(x) = P(+1 \\ \\big| \\ x) \\in [0,1] $\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{1+\\exp(-x)} = \\frac{ e^x }{1 + e^x } = \\theta( x ) = 1 - \\theta(-x)\n",
    "$$\n",
    "\n",
    "![img](imgs/c10-logistic-func-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理想狀況是有如下資料 (noiseless data):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_1, y'_1 & = 0.9 & = P(+1 \\ \\big| \\ x_1) \\\\\n",
    "x_2, y'_2 & = 0.2 & = P(+1 \\ \\big| \\ x_2) \\\\\n",
    "\\cdots \\\\\n",
    "x_N, y'_N & = 0.6 & = P(+1 \\ \\big| \\ x_N)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "實際狀況的資料，不知道機率，只知道 0 or 1 (有或無)；  \n",
    "就像有雜訊的資料 (noisy data):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x_1, y'_1 & = 1 \\\\\n",
    "x_2, y'_2 & = 0 \\\\\n",
    "\\cdots \\\\\n",
    "x_N, y'_N & = 0\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features $ x = (x_0, x_1, x_2, \\cdots, x_d) $\n",
    "\n",
    "獲得 weighted Risk Score: s\n",
    "\n",
    "$ s = \\sum_{i=0}^d w_i x_i $\n",
    "\n",
    "利用 Logistic Function $ \\theta $, 將分數 s 轉化爲 0~1 的機率: \n",
    "\n",
    "$$ h(x) = \\theta(w^T x) = \\frac{1}{1 + \\exp(- w^T x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Measure $ E_{in}(w) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target function: $ f(x) = P( +1 \\ \\big| \\ x) $  \n",
    "可以寫成:\n",
    "\n",
    "$ P(\\ y\\ \\big| \\ x\\ ) = \n",
    "\\begin{cases}\n",
    "f(x) & \\text{ for } y = +1 \\\\\n",
    "1 - f(x) & \\text{ for } y = -1 \\\\\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "若有一組資料 $ D = \\big\\{ (x_1, +1), (x_2, -1), \\cdots, (x_N, -1) \\big\\} $,  \n",
    "資料 D 發生的機率是:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "P(x_1) \\ P(+1 \\ | \\ x_1) & \\times P(x_2) \\ P(-1 \\ | \\ x_2) & \\cdots & \\times P(x_N) \\ P(-1 \\ | \\ x_N) \\\\\n",
    "= P(x_1) \\ f(x_1) & \\times P(x_2) \\ (1-f(x_2)) & \\cdots & \\times P(x_N) \\ (1-f(x_N))\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若 hypothesis h 很接近 target function f, 那麼 g 應該可讓上式發生機率最大。  \n",
    "\n",
    "$ g = \\text{argmax}_h \\ \\ \\text{likelihood}(h) $\n",
    "\n",
    "logistic function 的對稱性: $ 1 - h(x) = h(-x) $\n",
    "\n",
    "$ P(x_1) \\ h(x_1) \\ \\times \\ P(x_2) \\ (1-h(x_2)) \\ \\cdots \\ \\times \\ P(x_N) \\ (1-h(x_N)) $\n",
    "\n",
    "$ = P(x_1) \\ h(+x_1) \\ \\times \\ P(x_2) \\ (h(-x_2)) \\ \\cdots \\ \\times \\ P(x_N) \\ h(-x_N) $\n",
    "\n",
    "$ \\varpropto \\prod_{n=1}^N h(y_n x_n) $\n",
    "\n",
    "\n",
    "希望最大化 h 產生資料組 D 的機率:\n",
    "$$ \\max_h \\text{likelihood(h)} \\varpropto \\prod_{n=1}^N h(y_n x_n) = $$\n",
    "\n",
    "$$ \\max_w \\text{likelihood(w)} \\varpropto \\prod_{n=1}^N \\theta \\big(y_n w^T x_n \\big) \\to $$\n",
    "\n",
    "$$ \\max_w \\ln \\prod_{n=1}^N \\theta \\big(y_n w^T x_n \\big) = $$\n",
    "\n",
    "$$ \\max_w \\sum_{n=1}^N \\ln \\theta \\big(y_n w^T x_n \\big) \\to $$\n",
    "\n",
    "$$ \\min_w \\sum_{n=1}^N - \\ln \\theta \\big(y_n w^T x_n \\big) \\to $$\n",
    "\n",
    "$$ \\min_w \\frac{1}{N} \\sum_{n=1}^N - \\ln \\theta \\big(y_n w^T x_n \\big) \\to $$\n",
    "\n",
    "$$ \\min_w \\frac{1}{N} \\sum_{n=1}^N \\ln \\big(1 + \\exp(-y_n w^T x_n) \\big) \\to $$\n",
    "\n",
    "$$ \\min_w \\underbrace{ \\frac{1}{N} \\sum_{n=1}^N \\text{err} \\big(w, x_n, y_n \\big)}_{E_{in}(w)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy Error\n",
    "\n",
    "$$ \\text{err}(w,x,y) = \\ln \\big( 1 + \\exp(-ywx) \\big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing $ E_{in}(w) $ - Gradient\n",
    "\n",
    "$ E_{in}(w) $ is Continuous, Differentiable, Twice-Differentiable, Convex.\n",
    "\n",
    "Find $ \\nabla E_{in}(w) = 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to derive $ \\nabla E_{in}(w) $, use Chain Rule:\n",
    "\n",
    "$$ E_{in}(w) = \\frac{1}{N} \\sum_{n=1}^N \\ln \\Big( \\underbrace{1 + \\overbrace{\\exp(-y_n w^T x_n)}^{b}}_{a} \\Big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\frac{\\partial E_{in}(w)}{\\partial_{w_i}} =\n",
    "\\frac{1}{N} \\sum_{n=1}^{N} \\Big( \\frac{\\partial \\ln(a)}{\\partial_a} \\Big) \\ \n",
    "\\Big( \\frac{\\partial(1+\\exp(b))}{\\partial_b} \\Big) \\ \n",
    "\\Big( \\frac{\\partial(- y_n w^T x_n)}{\\partial_{w_i}} \\Big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{N} \\sum_{n=1}^{N} \\Big( \\frac{1}{a} \\Big) \\ \n",
    "\\Big( \\exp(b) \\Big) \\ \n",
    "\\Big( -y_n x_{n,i} \\Big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{N} \\sum_{n=1}^{N} \\Big( \\frac{\\exp(b)}{1 + \\exp(b)} \\Big) \\ \n",
    "\\Big( -y_n x_{n,i} \\Big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{N} \\sum_{n=1}^{N} \\theta \\Big( b \\Big) \\ \n",
    "\\Big( -y_n x_{n,i} \\Big)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla E_{in}(w) = \\frac{1}{N} \\sum_{n=1}^{N} \\theta \\Big( - y_n w^T x_n \\Big) \\ \n",
    "\\Big( -y_n x_n \\Big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLA Revisited: Iterative Optimization\n",
    "\n",
    "pick some n, and update $ w_t $ by\n",
    "\n",
    "$ w_{t+1} = w_t + \\underbrace{1}_{\\eta} \\times \\underbrace{\\text{boolean} \\Big( \\text{sign}( w_t^T x_n ) \\ne y_n  \\Big) \\cdot y_n x_n}_{v} $\n",
    "\n",
    "When stop, return last w as g.\n",
    "\n",
    "Choice of $ \\big( \\eta, v \\big) $ and stopping condition defines **Iterative Optimization Approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Approximation\n",
    "\n",
    "A greedy approach for some given $ \\eta \\gt 0 $\n",
    "\n",
    "$$ \\min_{\\Vert v \\Vert = 1} \\ \\ \\ E_{in}(w_t + \\eta v) $$\n",
    "\n",
    "Local approximation by linear formula makes problem easier.  \n",
    "將整段 $ E_{in} $ 分成小段較好處理，分成小段是拿一點加上一小距離: 斜率 $ \\times v $\n",
    "\n",
    "$$ E_{in}(w_t + \\eta v) \\approx E_{in}(w_t) + \\eta v^T \\nabla E_{in} (w_t) $$\n",
    "\n",
    "if $ \\eta $ really small (Taylor Expansion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\min_{\\Vert v \\Vert = 1} \\Big( \\ \\underbrace{ E_{in}(w_t) }_\\text{known} + \\underbrace{ \\eta }_{\\text{given positive}} v^T \\underbrace{ \\nabla E_{in} (w_t) }_\\text{known} \\Big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\to \\min_{\\Vert v \\Vert = 1} \\Big( \\ v^T \\underbrace{ \\nabla E_{in} (w_t) }_\\text{known} \\Big)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**optimal v**: 與向量 $ v^T $ 完全反方向的 $ \\nabla E_{in}(w_t) $ 會獲得最小化，所以最佳化的\n",
    "\n",
    "$$\n",
    "v = - \\frac{\\nabla E_{in}(w_t)}{\\Vert \\nabla E_{in}(w_t) \\Vert}\n",
    "$$\n",
    "\n",
    "**Gradient descent**: for small $ \\eta $ \n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\eta \\frac{\\nabla E_{in}(w_t)}{\\Vert \\nabla E_{in}(w_t) \\Vert}\n",
    "$$\n",
    "\n",
    "**Gradient descent**: a Simple and Popular optimization tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of $ \\eta $\n",
    "\n",
    "$ \\eta $ better be monotonic of $ \\Vert \\nabla E_{in}(w_t) \\Vert $,\n",
    "\n",
    "$ \\eta $ 最好是與梯度同比例調整大小，因此令 $ \\eta'= \\frac{\\eta}{\\Vert \\nabla E_{in}(w_t) \\Vert} $\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\eta \\frac{\\nabla E_{in}(w_t)}{\\Vert \\nabla E_{in}(w_t) \\Vert}\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\eta' \\nabla E_{in}(w_t)\n",
    "$$\n",
    "\n",
    "$ \\eta' $ 也稱為 Fixed Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Algorithm\n",
    "\n",
    "Initialize $ w_0 $,\n",
    "\n",
    "For t = 0, 1, ...\n",
    "\n",
    "#### STEP ONE:\n",
    "\n",
    "Compute\n",
    "\n",
    "$$\n",
    "\\nabla E_{in}(w_t) = \\frac{1}{N} \\sum_{n=1}^N \\theta \\Big( -y_n w_t^T x_n \\Big) \\big( -y_n x_n \\big)\n",
    "$$\n",
    "\n",
    "#### STEP TWO:\n",
    "\n",
    "Update by\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\eta \\nabla E_{in}(w_t)\n",
    "$$\n",
    "\n",
    "Until $ \\nabla E_{in}(w_{t+1}) \\approx 0 $ or **Enough Iterations**,\n",
    "\n",
    "return last $ w_{t+1} $ as g."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
