{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "### Shallow versus Deep Neural Networks\n",
    "\n",
    "Shallow NNet: few (hidden) layers  \n",
    "    - more efficient to train\n",
    "    - simpler structural decisions\n",
    "    - theoretically powerful enough\n",
    "    \n",
    "Deep NNet: many layers\n",
    "    - challenging to train\n",
    "    - sophisticated structural decisions\n",
    "    - 'arbitrarily' powerful\n",
    "    - more 'meaningful'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meaningfulness of Deep Learning\n",
    "\n",
    "- 每層需要負責的處理較少。\n",
    "- 能夠處理對困難度高的 raw feature 學習問題，如影像/音頻\n",
    "\n",
    "### Challenges and Key Techniques\n",
    "\n",
    "- Difficult structural decisions:\n",
    "    - 如何決定神經元之間的結構？\n",
    "    - subjective with domain knowledge: like **Convolutional NNet** for images\n",
    "    - 相近的圖片像素才鏈接到下個神經元，太遠的像素不鏈接；相近的像素間才有意義。\n",
    "- High model complexity:\n",
    "    - 有眾多的轉換, 與權重w 要處理\n",
    "    - no big worries if *data is big enough*\n",
    "    - regularization towards noise-tolerant: like 'dropout' (when network corrupted), 'denoising' (when input corrupted)\n",
    "- Hard optimization problem:\n",
    "    - 困難的最佳化，不是 convex, 可能落在 local minimum 上。\n",
    "    - careful initialization to avoid bad local minimum: 'pre-training'\n",
    "- Huge computational complexity\n",
    "    - novel hardware/architecture: like mini-batch with GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-Step Deep Learning Framework\n",
    "\n",
    "#### STEP ONE\n",
    "\n",
    "for $ \\mathscr{l} = 1, \\cdots, L, \\text{ pre-train } \\big\\{ w_{ij}^{(\\mathscr{l})} \\big\\} \\text{ assuming } w_{*}^{(1)}, \\cdots, w_{*}^{(\\mathscr{l} - 1)} \\text{ fixed. } $\n",
    "\n",
    "#### STEP TWO\n",
    "\n",
    "train with **backprop** on pre-trained NNet to fune-tune all $ \\big\\{ w_{ij}^{(\\mathscr{l})} \\big\\} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information-Preserving Encoding\n",
    "\n",
    "- weights: feature transform, i.e. encoding.\n",
    "- good weights: information-preserving encoding; next layer same info, with different representation.\n",
    "- information-preserving: decode accurately after encoding\n",
    "\n",
    "### Information-Preserving Neural Network\n",
    "\n",
    "<img src=\"imgs/c213-info-preserve-nnet.jpg\" style=\"width:500px\" />\n",
    "\n",
    "- Autoencoder: $ d \\to \\tilde{d} \\to d $ NNet with goal $ g_i(x) \\approx x_i $ : learn to approximate **identity function**\n",
    "- encoding weights: $ w_{ij}^{(1)} $\n",
    "- decoding weights: $ w_{ji}^{(2)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic autoencoder 獲得初始權重\n",
    "\n",
    "$ d \\to \\tilde{d} \\to d $ NNet with error function $ \\sum_{i=1}^d \\big( g_i(x) - x_i \\big)^2 $\n",
    "\n",
    "- backprop easily applies; shallow and easy to train\n",
    "- usually $ \\tilde{d} \\lt d $: compressed representation\n",
    "- data: $ \\big\\{ (x_1,y_1 = x_1), \\ \\ (x_2,y_2 = x_2), \\ \\cdots,(x_N,y_N = x_N)  \\big\\} $\n",
    "    - often categorized as unsupervised learning technique\n",
    "- sometimes constrain $ w_{ij}^{(1)} = w_{ji}^{(2)} $ as regularization.\n",
    "    - more sophisticated in calculation gradient\n",
    "\n",
    "basic autoencoder in basic deep learning, $ \\big\\{  w_{ij}^{(1)} \\big\\} $ taken as **shallowly pre-trained weights**\n",
    "\n",
    "Many successful pre-training techniques take 'fancier' autoencoders with different architectures &amp; regularization schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization in Deep Learning\n",
    "\n",
    "high model complexity: regularization needed:\n",
    "\n",
    "- structural decisions / constraints.\n",
    "- weight decay or weight elimination regularizers\n",
    "- early stopping\n",
    "- add artificial noise\n",
    "\n",
    "### denoising autoencoder:\n",
    "\n",
    "run basic autoencoder with data:\n",
    "\n",
    "$ \\big\\{ (\\tilde{x}_1,y_1 = x_1), \\ \\ (\\tilde{x}_2,y_2 = x_2), \\ \\cdots,(\\tilde{x}_N,y_N = x_N)  \\big\\} $\n",
    "\n",
    "where $ \\tilde{x}_n = x_n + $ artificial noise\n",
    "\n",
    "- often used instead of basic autoencoder in deep learning.\n",
    "- useful for data/image processing: $ g(\\tilde{x}) $ a denoising version of $ \\tilde{x} $\n",
    "- effect: 'constrain/regularize' g toward noise-tolerant denoising.\n",
    "\n",
    "Artificial noise / hint as regularization - pratically also useful for other NNet / models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "之前討論的是 nonlinear encoder, linear hypothesis for k-th component:\n",
    "\n",
    "$$\n",
    "h_k(x) = \\sum_{j=0}^{\\tilde{d}} w_{jk}^{(2)} \\ tanh \\Big( \\sum_{i=0}^d w_{ij}^{(1)} x_i \\Big)\n",
    "$$\n",
    "\n",
    "換成線性的 encoder, 就是將 tanh(.) 去掉:\n",
    "\n",
    "$$\n",
    "h_k(x) = \\sum_{j=0}^{\\tilde{d}} w_{jk}^{(2)} \\ \\Big( \\sum_{i=0}^d w_{ij}^{(1)} x_i \\Big)\n",
    "$$\n",
    "\n",
    "考慮幾個特例後，式子轉變成:\n",
    "\n",
    "- exclude $ x_0 $: range of i same as range of k\n",
    "- constrain $ w_{ij}^{(1)} = w_{ji}^{(2)} = w_{ij} $ : regularization\n",
    "    - denote $ W = [w_{ij}] \\text{ of size } d \\times \\tilde{d} $\n",
    "- assume $ \\tilde{d} \\lt d $ : ensure non-trival solution\n",
    "\n",
    "\n",
    "$$\n",
    "h_k(x) = \\sum_{j=0}^{\\tilde{d}} w_{kj} \\ \\Big( \\sum_{i=1}^d w_{ij} x_i \\Big) \\\\\n",
    "h(x) = W \\ W^T \\vec{x}\n",
    "$$\n",
    "\n",
    "如果要找出最好的 hypothesis, 就是對 W 矩陣做最佳化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Autoencoder Error Function\n",
    "\n",
    "最佳化的 錯誤定義，就是 \"編碼再解碼\" 後，數值盡量是不變的。\n",
    "\n",
    "$$\n",
    "E_{in}(h) = E_{in}(W) = \\frac{1}{N} \\sum_{n=1}^N \\Big\\Vert x_n - W W^T x_n \\Big\\Vert^2 \\ \\ , \\ \\  W: d \\times \\tilde{d}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eigen-decompose $ W \\ W^T = V \\ \\Gamma \\ V^T $  \n",
    "$ \\Gamma $ : 做了 eigenvalue 轉換後，中間得到的 eigenvalue 值\n",
    "\n",
    "V 是 d &times; d orthogonal matrix: $ V V^T = V^T V = I_d $  \n",
    "\n",
    "$ \\Gamma $ 是 d &times; d diagonal matrix with $ \\le \\tilde{d} $ non-zero,  \n",
    "因為 $ \\Gamma $ 是 $ W \\ W^T $ 的 eigen-decomposition, W 大小是 $ d \\times \\tilde{d} $, rank 最多是 $ \\tilde{d} $\n",
    "\n",
    "> 特殊矩陣的特徵分解 - 對稱矩陣  \n",
    "> 任意的 N×N 實對稱矩陣都有 N 個線性無關的特徵向量。並且這些特徵向量都可以正交單位化而得到一組正交且模為 1 的向量。  \n",
    "> 故實對稱矩陣 A 可被分解成 $ A = Q \\ \\Lambda Q^T $  \n",
    "> 其中 Q 為 正交矩陣， Λ 為實對角矩陣\n",
    "\n",
    "以上關係，可以推導下面的物理意義:\n",
    "\n",
    "$ W W^T x_n = V \\Gamma V^T x_n $\n",
    "\n",
    "$ V^T x_n $: 是乘以一個 orthonormal basis, 意思是轉換座標，如旋轉 rotate，或鏡像 relect  \n",
    "$ \\Gamma $: 是對角矩陣，最多有 $ \\tilde{d} $ 個不為零的數，而 $ d \\gt \\tilde{d} $, 所以乘上 $ \\Gamma $ 就是消去某些座標軸的向量，然後再放縮 scaling.  \n",
    "$ V (.) $ : 再把 旋轉 rotate，或鏡像 relect 的轉換座標復原回來。\n",
    "\n",
    "$ x_n = V \\ I \\ V^T x_n $: 將 $ x_n $ 轉換座標再復原回來，沒有變化。\n",
    "\n",
    "所以上面最佳化的 $ E_{in} $ 就轉變成了對 $ V, \\Gamma $ 的最佳化問題，知道這兩個，就知道最佳的 W 會是什麼。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Optimal $ \\Gamma $\n",
    "\n",
    "$$\n",
    "\\min_{V} \\min_{\\Gamma} \\frac{1}{N} \\sum_{n=1}^N \\Big\\Vert V I V^T x_n - V \\Gamma V^T x_n \\Big\\Vert^2\n",
    "$$\n",
    "\n",
    "假設先固定 V, 處理最小化的 $ \\Gamma $,  \n",
    "座標轉換 V 如 旋轉或鏡像 是不會影像長度的，所以劃去 V\n",
    "\n",
    "$$\n",
    "\\min_{\\Gamma} \\sum_{n=1}^N \\Big\\Vert \\big(I - \\Gamma \\big) V^T x_n \\Big\\Vert^2\n",
    "$$\n",
    "\n",
    "$ \\Gamma $ 是最多 $\\tilde{d}$ 個數不為零的對角矩陣，所以如要最小化 $ ( I - \\Gamma ) $，應該是希望相減結果越多零約好，  \n",
    "也就是 $ \\Gamma $ 中越多數為 1 越好 (最多 $ \\tilde{d} $ 個 1), 所以最佳的 $ \\Gamma $:\n",
    "\n",
    "$$\n",
    "optimal \\ \\Gamma =\n",
    "\\begin{bmatrix}\n",
    "I_{\\tilde{d}} & 0 \\\\\n",
    "0             & 0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著處理最小化 V 的部份:\n",
    "\n",
    "$$\n",
    "\\min_V \\sum_{n=1}^N \\Big\\Vert\n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & I_{d - \\tilde{d}}\n",
    "\\end{bmatrix}\n",
    "\\ V^T \\ x_n\n",
    "\\Big\\Vert^2\n",
    "$$\n",
    "\n",
    "可將上面的 min 轉換看成是  \n",
    "向量:$ V^T x_n $ 中要 [留下] 哪些維度，可以 [最小化] 長度。等同於:  \n",
    "向量:$ V^T x_n $ 中要 [拿掉] 哪些維度，可以 [最大化] 長度。\n",
    "\n",
    "$$\n",
    "\\max_V \\sum_{n=1}^N \\Big\\Vert\n",
    "\\begin{bmatrix}\n",
    "I_{\\tilde{d}} & 0 \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "\\ V^T \\ x_n\n",
    "\\Big\\Vert^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先看一個例子，$ \\tilde{d} = 1 $, 則 $ V^T x_n $ 的運算中，只有 V 的 first row: $ v^T $ 有用。  \n",
    "最大化算式變成 (v 是 orthonormal ):\n",
    "\n",
    "$$\n",
    "\\max_v \\sum_{n=1}^N v^T x_n x_n^T v, \\text{ subject to } v^T v = 1\n",
    "$$\n",
    "\n",
    "依據 lagrange multiplier 的原理，上式個別微分，會平行(等比例): $ \\lambda $  \n",
    "最佳化的 v 滿足 $  \\sum_{n=1}^N x_n x_n^T v = \\lambda v $\n",
    "\n",
    "最佳化的 v 就是 topmost eigenvector of $ X^T X $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般化的來看，$ \\tilde{d}: \\big\\{ v_j \\big\\}_{j=1}^{\\tilde{d}} $, 'topmost' eigenvectors of $ X^T X $\n",
    "\n",
    "### Linear autoencoder: projecting to orthogonal patterns w\n",
    "\n",
    "projecting to orthogonal patterns $ \\{ w_j \\} $ that matches $ \\{ x_n \\} $ most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "Linear Autoencoder or PCA\n",
    "\n",
    "STEP 1 - let $ \\overline{x} = avg(x_n) $, and let $ x_n \\leftarrow x_n - \\overline{x} $, 減去平均數\n",
    "\n",
    "STEP 2 - calculate $ \\tilde{d} $ top eigenvectors $ w_1, w_2, \\cdots, w_{\\tilde{d}} $ of $ X^T X $\n",
    "\n",
    "STEP 3 - return feature transform $ \\Phi(x) = W(x - \\overline{x}) $\n",
    "\n",
    "- Linear autoencoder: $ max \\sum \\big( \\text{ magnitude after projection} \\big)^2 $\n",
    "- PCA from stat.: $ max \\sum \\big( \\text{ variance after projection} \\big)^2 $\n",
    "- Both useful for linear dimension reduction."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
