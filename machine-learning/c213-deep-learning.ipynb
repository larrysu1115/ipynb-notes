{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "### Shallow versus Deep Neural Networks\n",
    "\n",
    "Shallow NNet: few (hidden) layers  \n",
    "    - more efficient to train\n",
    "    - simpler structural decisions\n",
    "    - theoretically powerful enough\n",
    "    \n",
    "Deep NNet: many layers\n",
    "    - challenging to train\n",
    "    - sophisticated structural decisions\n",
    "    - 'arbitrarily' powerful\n",
    "    - more 'meaningful'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meaningfulness of Deep Learning\n",
    "\n",
    "- 每層需要負責的處理較少。\n",
    "- 能夠處理對困難度高的 raw feature 學習問題，如影像/音頻\n",
    "\n",
    "### Challenges and Key Techniques\n",
    "\n",
    "- Difficult structural decisions:\n",
    "    - 如何決定神經元之間的結構？\n",
    "    - subjective with domain knowledge: like **Convolutional NNet** for images\n",
    "    - 相近的圖片像素才鏈接到下個神經元，太遠的像素不鏈接；相近的像素間才有意義。\n",
    "- High model complexity:\n",
    "    - 有眾多的轉換, 與權重w 要處理\n",
    "    - no big worries if *data is big enough*\n",
    "    - regularization towards noise-tolerant: like 'dropout' (when network corrupted), 'denoising' (when input corrupted)\n",
    "- Hard optimization problem:\n",
    "    - 困難的最佳化，不是 convex, 可能落在 local minimum 上。\n",
    "    - careful initialization to avoid bad local minimum: 'pre-training'\n",
    "- Huge computational complexity\n",
    "    - novel hardware/architecture: like mini-batch with GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-Step Deep Learning Framework\n",
    "\n",
    "#### STEP ONE\n",
    "\n",
    "for $ \\mathscr{l} = 1, \\cdots, L, \\text{ pre-train } \\big\\{ w_{ij}^{(\\mathscr{l})} \\big\\} \\text{ assuming } w_{*}^{(1)}, \\cdots, w_{*}^{(\\mathscr{l} - 1)} \\text{ fixed. } $\n",
    "\n",
    "#### STEP TWO\n",
    "\n",
    "train with **backprop** on pre-trained NNet to fune-tune all $ \\big\\{ w_{ij}^{(\\mathscr{l})} \\big\\} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information-Preserving Encoding\n",
    "\n",
    "- weights: feature transform, i.e. encoding.\n",
    "- good weights: information-preserving encoding; next layer same info, with different representation.\n",
    "- information-preserving: decode accurately after encoding\n",
    "\n",
    "### Information-Preserving Neural Network\n",
    "\n",
    "<img src=\"imgs/c213-info-preserve-nnet.jpg\" style=\"width:500px\" />\n",
    "\n",
    "- Autoencoder: $ d \\to \\tilde{d} \\to d $ NNet with goal $ g_i(x) \\approx x_i $ : learn to approximate **identity function**\n",
    "- encoding weights: $ w_{ij}^{(1)} $\n",
    "- decoding weights: $ w_{ji}^{(2)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic autoencoder 獲得初始權重\n",
    "\n",
    "$ d \\to \\tilde{d} \\to d $ NNet with error function $ \\sum_{i=1}^d \\big( g_i(x) - x_i \\big)^2 $\n",
    "\n",
    "- backprop easily applies; shallow and easy to train\n",
    "- usually $ \\tilde{d} \\lt d $: compressed representation\n",
    "- data: $ \\big\\{ (x_1,y_1 = x_1), \\ \\ (x_2,y_2 = x_2), \\ \\cdots,(x_N,y_N = x_N)  \\big\\} $\n",
    "    - often categorized as unsupervised learning technique\n",
    "- sometimes constrain $ w_{ij}^{(1)} = w_{ji}^{(2)} $ as regularization.\n",
    "    - more sophisticated in calculation gradient\n",
    "\n",
    "basic autoencoder in basic deep learning, $ \\big\\{  w_{ij}^{(1)} \\big\\} $ taken as **shallowly pre-trained weights**\n",
    "\n",
    "Many successful pre-training techniques take 'fancier' autoencoders with different architectures &amp; regularization schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization in Deep Learning\n",
    "\n",
    "high model complexity: regularization needed:\n",
    "\n",
    "- structural decisions / constraints.\n",
    "- weight decay or weight elimination regularizers\n",
    "- early stopping\n",
    "- add artificial noise\n",
    "\n",
    "### denoising autoencoder:\n",
    "\n",
    "run basic autoencoder with data:\n",
    "\n",
    "$ \\big\\{ (\\tilde{x}_1,y_1 = x_1), \\ \\ (\\tilde{x}_2,y_2 = x_2), \\ \\cdots,(\\tilde{x}_N,y_N = x_N)  \\big\\} $\n",
    "\n",
    "where $ \\tilde{x}_n = x_n + $ artificial noise\n",
    "\n",
    "- often used instead of basic autoencoder in deep learning.\n",
    "- useful for data/image processing: $ g(\\tilde{x}) $ a denoising version of $ \\tilde{x} $\n",
    "- effect: 'constrain/regularize' g toward noise-tolerant denoising."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
