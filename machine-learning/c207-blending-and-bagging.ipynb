{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending and Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若使用各種不同的 algorithm, model 獲得多個 $ g: g_1, g_2, \\cdots, g_T $ 時，如何綜合起來獲得最後的 g_t(s) 呢？  \n",
    "可能有以下幾種方式:\n",
    "\n",
    "ONE - select: 選擇最好的 $ E_{val}(g_t^{-}) $ 最小的  \n",
    "$ G(x) = g_{t*}(x), t_{*} = argmin_{t \\in \\{ 1,2,\\cdots,T \\}} E_{val}(g_t^{-}) $\n",
    "\n",
    "TWO - mix uniformly: 平等看待每一個 g，綜合起來  \n",
    "$ G(x) = sign \\big( \\sum_{t=1}^T 1 \\cdot g_t(x) \\big) $\n",
    "\n",
    "THREE - mix non-uniformly: 給每一個 g 加權分數後，綜合起來  \n",
    "$ G(x) = sign \\big( \\sum_{t=1}^T \\alpha_t \\cdot g_t(x) \\big), \\ \\ \\alpha \\ge 0 $\n",
    "\n",
    "FOUR - 利用自訂的 constaint 限制每個 g 的使用  \n",
    "$ G(x) = sign \\big( \\sum_{t=1}^T q_t(x) \\cdot g_t(x) \\big), \\ \\ q_t(X) \\ge 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Blending (Voting) for Classification\n",
    "\n",
    "mix uniformly: 平等看待每一個 g，綜合起來\n",
    "\n",
    "$$ G(x) = sign \\big( \\sum_{t=1}^T 1 \\cdot g_t(x) \\big) $$\n",
    "\n",
    "若是每一個小 g 都很類似，結果 G 沒有特別不同。\n",
    "\n",
    "若是小 g 差異大，majority can correct minority，如同 collective intelligence 集體智慧。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Blending for Regression\n",
    "\n",
    "$$\n",
    "G(x) = \\frac{1}{T} \\sum_{t=1}^T g_t(x)\n",
    "$$\n",
    "\n",
    "average **could be** more accurate than individual.\n",
    "\n",
    "#### Diverse Hypotheses:\n",
    "\n",
    "Even simple uniform blending can be better than any **single hypothesis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Blending\n",
    "\n",
    "known $ g_t $, each to be given $ \\alpha_t $ ballot\n",
    "\n",
    "$$\n",
    "G(x) = sign \\Big( \\sum_{t=1}^T \\alpha_t g_t(x) \\Big), \\ \\Big| \\ \\ \\alpha_t \\ge 0\n",
    "$$\n",
    "\n",
    "computing 'good' $ \\alpha_t : \\min_{\\alpha_t \\ge 0} E_{in}(\\alpha) $\n",
    "\n",
    "#### Linear Blending for regression\n",
    "\n",
    "$\n",
    "\\min_{\\alpha_t \\ge 0} \\frac{1}{N} \\sum_{n=1}^N \\Big( y_n - \\sum_{t=1}^T \\alpha_t g_t(x_n) \\Big)^2\n",
    "$\n",
    "\n",
    "#### LinReg + Transformation\n",
    "\n",
    "$\n",
    "\\min_{w_i} \\frac{1}{N} \\sum_{n=1}^N \\Big( y_n - \\sum_{i=1}^{\\tilde{d}} w_i \\Phi_i(x_n) \\Big)^2\n",
    "$\n",
    "\n",
    "Linear Blending for regression is like two-level learning.\n",
    "\n",
    "Linear Blending = LinModel + hypotheses as transform + constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Blending for Binary classification\n",
    "\n",
    "二元分類，只有 是/否。若知道 g 有 99% 機會是錯的，則 -g 就有 99% 機會是對的。如此可以忽略掉 constraint : $ \\alpha \\ge 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Blending versus Selection\n",
    "\n",
    "like selection, blending practically done with:\n",
    "\n",
    "- $ E_{val} $ instead of $ E_{in} $\n",
    "- $ g_t^- $ from minimum $ E_{train} $\n",
    "\n",
    "先從訓練資料 $ D_{train} $ 獲得一堆的 $ g_1^-, g_2^-, \\cdots, g_T^- $ ；\n",
    "\n",
    "然後將 validation data $ D_{val}: (x_n, y_n) $ 轉換成 $ \\Big( z_n = \\Phi^-(x_n), y_n \\Big) $, where $ \\Phi^-(x) = \\big( g_1^-(x), \\cdots, g_T^-(x) \\big) $\n",
    "\n",
    "#### Linear Blending Steps\n",
    "\n",
    "接著 Linear blending 可以:\n",
    "\n",
    "1. compute $ \\alpha = Lin \\Big( \\ \\big\\{ \\ (z_n, y_n)  \\ \\big\\}  \\  \\Big) $\n",
    "2. return $ G_{LINB}(x) = LinH \\Big( \\ innerprod \\big( \\ \\alpha, \\Phi(x)  \\ \\big)  \\  \\Big) $\n",
    "\n",
    "注意到最後使用的是 $ \\Phi(x) = \\big( g_1(x), g_2(x), \\cdots, g_T(x) \\big) $, 是整體資料做出的；  \n",
    "不是 $ \\Phi^-, g^- $ 只有 測試 v-fold 資料做出的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Any Blending （Stacking)\n",
    "\n",
    "1. compute $ \\tilde{g} = Any \\Big( \\ \\big\\{ \\ (z_n, y_n)  \\ \\big\\}  \\  \\Big) $\n",
    "2. return $ G_{ANYB}(x) = \\tilde{g} \\Big( \\ \\Phi(x)  \\ \\Big) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g - diversity\n",
    "\n",
    "diversity is important for $ g_t $ aggregation\n",
    "\n",
    "- diversity by different models: $ g_1 \\in H_1, g_2 \\in H_2, \\cdots $\n",
    "- diversity by different parameters: GD with $ \\eta = 0.01, 0.001, 0.002 $\n",
    "- diversity by algorithm randomness: PLA 起始不同就有不同的 random 效果。\n",
    "- diversity by data randomness: Bootstrap (select with replacement)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
