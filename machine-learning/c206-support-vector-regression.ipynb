{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regression\n",
    "\n",
    "如何結合 L2-Regularized Linear Model 與 Kernel, 獲得 Analytic Solution for kernel ridge regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for any L2-regularized linear model\n",
    "\n",
    "$$\n",
    "\\min_w \\frac{\\lambda}{N} w^T w + \\frac{1}{N} \\sum_{n=1}^N err \\big( y_n, w^T z_n \\big)\n",
    "$$\n",
    "\n",
    "optimal $ w_{*} = \\sum_{n=1}^N \\beta_n z_n $\n",
    "\n",
    "for Regression with Squared Error:\n",
    "\n",
    "$$\n",
    "err \\big( y_n, w^T z_n \\big) = \\big( y - w^T z \\big)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Ridge Regression Problem\n",
    "\n",
    "$\n",
    "\\min_w \\frac{\\lambda}{N} w^T w + \\frac{1}{N} \\sum_{n=1}^N \\big( y_n - w^T z_n \\big)^2\n",
    "$\n",
    "\n",
    "optimal $ w_{*} = \\sum_{n=1}^N \\beta_n z_n $\n",
    "\n",
    "將 w 替換成 $ \\beta $, 就可以將問題變成 $ \\beta $ 的最佳化。 \n",
    "\n",
    "$$\n",
    "\\min_{\\beta} \\frac{\\lambda}{N} \\sum_{n=1}^N \\sum_{m=1}^N \\beta_n \\beta_m K(x_n, x_m) \n",
    "+ \\frac{1}{N} \\sum_{n=1}^N \\Big( y_n - \\sum_{m=1}^N \\beta_m  K(x_n,x_m) \\Big)^2 \\\\\n",
    "= \\frac{\\lambda}{N} \\beta^T K \\beta + \\frac{1}{N} \\big( \\beta^T K ^T K \\beta - 2 \\beta^T K^T y + y^T y \\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving Kernel Ridge Regression\n",
    "\n",
    "$\n",
    "E_{aug}(\\beta) = \\frac{\\lambda}{N} \\beta^T K \\beta + \\frac{1}{N} \\big( \\beta^T K ^T K \\beta - 2 \\beta^T K^T y + y^T y \\big)\n",
    "$\n",
    "\n",
    "$\n",
    "\\nabla E_{aug}(\\beta) = \\frac{2}{N} \\big( \\lambda K^T I \\beta + K^T K \\beta - k^T y \\big) \\\\\n",
    "= \\frac{2}{N} K^T \\big( ( \\lambda I + K ) \\beta - y \\big)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "want $ \\nabla E_{aug} (\\beta) = 0 $ : one anayttic solution\n",
    "\n",
    "$$\n",
    "\\beta = \\big( \\lambda I + K \\big)^{-1} y\n",
    "$$\n",
    "\n",
    "- $ (...)^{-1} $ always exists for $ \\lambda \\gt 0 $, because K is positive semi-definite. (Mercer's condition)\n",
    "- time complexity: $ O(N^3) $ with simple dense matrix inversion\n",
    "\n",
    "Can now do non-linear regression 'easily'.\n",
    "\n",
    "![img](imgs/c206-linear-kernel-ridge-reg.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft-Margin SVM versus Least-Squares SVM\n",
    "\n",
    "least-squares SVM (LSSVM) = kernel ridge regression for classification\n",
    "\n",
    "LSSVM 與 Soft-Margin Gauusian SVM 找出的邊界類似，但是 LSSVM 的 Support Vector 多很多。  \n",
    "\n",
    "- dense $ \\beta $ : LSSVM, kernel LogReg\n",
    "- sparse $ \\alpha $ : standard SVM\n",
    "\n",
    "因此 LSSVM 做預測時候較費時\n",
    "\n",
    "可否找到 sparse $ \\beta $ like standard SVM ?\n",
    "\n",
    "![img](imgs/c206-tube.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "忽略掉在 tube (紫色區域) 中的錯誤，設定紫色區域的高度是 $ 2 \\epsilon $，於是 error measure 變成:\n",
    "\n",
    "$$\n",
    "err(y, s) = \\max \\big( 0, | s - y | -\\epsilon \\big)\n",
    "$$\n",
    "\n",
    "這中衡量錯誤的方式 error measure 通常叫做 $ \\epsilon $-insensitive error with $ \\epsilon \\gt 0 $\n",
    "\n",
    "TODO: L2-regularized tube regression to get sparse $ \\beta $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2-Regularized Tube Regression\n",
    "\n",
    "$$\n",
    "\\min_w \\ \\ \\frac{\\lambda}{N} w^T w + \\frac{1}{N} \\sum_{n=1}^N \\max \\big( 0, |w^T z_n - y| - \\epsilon \\big)\n",
    "$$\n",
    "\n",
    "希望用 類似 standard SVM > QP > Dual 方式去找出 sparse solution. 於是微調上式加入 b, 改變係數...\n",
    "\n",
    "$$\n",
    "\\min_{b,w} \\ \\ \\frac{1}{2} w^T w + C \\sum_{n=1}^N \\max \\big( 0, |w^T z_n + b - y_n| - \\epsilon \\big)\n",
    "$$\n",
    "\n",
    "將 error measure 轉變成 $ \\xi_n $\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_{b,w,\\xi} \\ \\ & \\frac{1}{2} w^T w + C \\sum_{n=1}^N \\xi_n \\\\\n",
    "s.t. \\ \\ & \\big| w^T z_n + b - y_n \\big| \\le \\epsilon + \\xi_n \\\\\n",
    "& \\xi_n \\ge 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "making constraints linear, 去掉絕對值，將 $ \\xi $ 拆成了兩個:  \n",
    "$ \\xi^{\\wedge} $ : 在 tube 上面的違反量  \n",
    "$ \\xi^{\\vee} $ : 在 tube 下面的違反量\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_{b,w,\\xi} \\ \\ & \\frac{1}{2} w^T w + C \\sum_{n=1}^N \\big( \\xi_n^{\\vee} + \\xi_n^{\\wedge} \\big) \\\\\n",
    "s.t. \\ \\ & - \\epsilon - \\xi_n^{\\vee} \\le y_n - w^T z_n - b \\le \\epsilon + \\xi_n^{\\wedge} \\\\\n",
    "& \\xi_n^{\\wedge} \\ge 0 \\\\\n",
    "& \\xi_n^{\\vee} \\ge 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Support Vector Regression (SVR) primal:\n",
    "\n",
    "- minimize regularizer: $ w^T w $\n",
    "- upper tube violations $ \\xi_n^{\\wedge} $\n",
    "- lower tube violations $ \\xi_n^{\\vee} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Programming for SVR\n",
    "\n",
    "上式就是 SVR 的 QP prime 形式, 參數和複雜度如下:\n",
    "\n",
    "- 參數 C : trade-off between Regularization and Tube Violation\n",
    "- 參數 $ \\epsilon $ : vertical tube width, tube 的寬度\n",
    "- QP of $ \\tilde{d} + 1 + 2N $ variables, 2N + 2N constraints.\n",
    "\n",
    "接下來將 SVR primal 推導成 Dual, 即可移除掉對 $ \\tilde{d} $ 的依賴。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagrange Multipliers $ \\alpha^{\\vee}, \\alpha^{\\wedge} $\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{objective function} \\ \\ & \\frac{1}{2} w^T w + C \\sum_{n=1}^N \\big( \\xi_n^{\\vee} + \\xi_n^{\\wedge} \\big) \\\\\n",
    "\\text{lagrange multiplier} \\alpha_n^{\\wedge} \\ \\ & & y_n - w^T z_n - b & \\le \\epsilon + \\xi_n^{\\wedge} \\\\\n",
    "\\text{lagrange multiplier} \\alpha_n^{\\vee} \\ \\ & - \\epsilon - \\xi_n^{\\vee} \\le & y_n - w^T z_n - b & &  \\\\\n",
    "& \\xi_n^{\\wedge} \\ge 0 & & \\\\\n",
    "& \\xi_n^{\\vee} \\ge 0   & &\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Some of the KKT conditions\n",
    "\n",
    "To get w:\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_i} = 0 \\to \\\\\n",
    "w = \\sum_{n=1}^N \\underbrace{\\big( \\alpha_n^{\\wedge} - \\alpha_n^{\\vee} \\big)}_{\\beta_n} z_n\n",
    "$\n",
    "\n",
    "To get b:\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} = 0 \\to \\\\\n",
    "\\sum_{n=1}^N \\big( \\alpha_n^{\\wedge} - \\alpha_n^{\\vee} \\big) = 0\n",
    "$\n",
    "\n",
    "Complementary Slackness:\n",
    "\n",
    "$\n",
    "\\alpha_n^{\\wedge} \\big( \\epsilon + \\xi_n^{\\wedge} - y_n + w^T z_n + b \\big) = 0 \\\\\n",
    "\\alpha_n^{\\vee}   \\big( \\epsilon + \\xi_n^{\\vee}   + y_n - w^T z_n - b \\big) = 0\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Dual and SVR Dual\n",
    "\n",
    "![img](imgs/c206-svr-dual.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity of SVR solution\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_i} = 0 \\to \\\\\n",
    "w = \\sum_{n=1}^N \\underbrace{\\big( \\alpha_n^{\\wedge} - \\alpha_n^{\\vee} \\big)}_{\\beta_n} z_n\n",
    "$\n",
    "\n",
    "Complementary Slackness:\n",
    "\n",
    "$\n",
    "\\alpha_n^{\\wedge} \\big( \\epsilon + \\xi_n^{\\wedge} - y_n + w^T z_n + b \\big) = 0 \\\\\n",
    "\\alpha_n^{\\vee}   \\big( \\epsilon + \\xi_n^{\\vee}   + y_n - w^T z_n - b \\big) = 0\n",
    "$\n",
    "\n",
    "Sparsity : 什麼時候 $ \\beta $ 會是 0 ?\n",
    "\n",
    "如果是在 tube 內的點， $ \\big| w^T z_n + b - y_n \\big| \\lt \\epsilon $  \n",
    "$ \\to \\xi_n^{\\wedge} = 0, \\ \\  \\xi_n^{\\vee} = 0 $  \n",
    "$ \\to ( \\epsilon + \\xi_n^{\\wedge} - y_n + w^T z_n + b ) \\ne 0, \\ \\ ( \\epsilon + \\xi_n^{\\vee} + y_n - w^T z_n - b ) \\ne 0 $\n",
    "$ \\to \\alpha_n^{\\wedge} = 0, \\ \\  \\alpha_n^{\\vee} = 0 $  \n",
    "$ \\to \\beta = 0 $\n",
    "\n",
    "SVs $ ( \\beta \\ne 0 ) $ : on or outside tube.\n",
    "\n",
    "SVR: allows sparse $ \\beta $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map of Linear Models\n",
    "\n",
    "![img](imgs/c206-map-linear-models.png)\n",
    "\n",
    "### Map of Linear / Kernel Models\n",
    "\n",
    "![img](imgs/c206-map-lineark-models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 第一排: PLA/Pocket, linear SVR 實務上少用到，因為 worse performance.\n",
    "- 第三排: kernel ridge reg., kernel logistic reg. 實務上少用到，因為 dense $ \\beta $. 通常用第四排的對應方案。"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
