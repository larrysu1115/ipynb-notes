{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](imgs/c14-overfit-to-reg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將 高次多項式 的 Hypothesis 變回 低次多項式，  \n",
    "使的偏離 $ E_{out} $ 的 $ H_10 $ 變得平滑，貼近 $ H_2 $\n",
    "\n",
    "Historical Name: function approximation for **ill-posed problems**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepping Back as Constraint\n",
    "\n",
    "> note: denote $ \\tilde{w} $ by w, in this lecture.\n",
    "\n",
    "$ \\Phi_Q(x) = (1, x, x^2, \\cdots, x^Q) $\n",
    "\n",
    "Hypothesis w in $ H_{10} : w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\cdots + w_{10} x^{10} $\n",
    "\n",
    "Hypothesis w in $ H_{2\\ } : w_0 + w_1 x + w_2 x^2 + 0 x^3 + \\cdots + 0 x^{10} $\n",
    "\n",
    "將 $ w_3, \\cdots, w_{10} $ 都限制為 0，就可以將 10次的H 換成 2次的Hypothesis。\n",
    "\n",
    "然後使用 linear regression 去求 \n",
    "\n",
    "$$ \\min_{w \\in \\Re^{10+1}} E_{in}(w) $$\n",
    "\n",
    "延伸來看，只要有 8 個 $ w_i $ 為零，就是將 $ H_{10} $ 限縮到 $ H_{2} $，  \n",
    "$ H_{2} $ 可以是 $ w_0, w_1, \\cdots, w_{10} $\n",
    "\n",
    "原來的問題，加上限制後成為:\n",
    "\n",
    "$$ H_2' = \\Big \\{ w \\in \\Re^{10+1}, \\text{至少八個} w_q = 0 \\Big \\} $$\n",
    "\n",
    "求解:\n",
    "\n",
    "$$ \\min_{w \\in \\Re^{10+1}} E_{in}(w) $$\n",
    "\n",
    "$$ s.t. \\sum_{q=0}^{10} \\big[ w_q \\ne 0 \\big]_{bool} \\le 3 $$\n",
    "\n",
    "所以 $ H_2' $ 較 $ H_2 $ 有彈性，同時比 $ H_{10} $ 的 overfit 風險低。\n",
    "\n",
    "$$ H_2 \\subset H_2' \\subset H_{10} $$\n",
    "\n",
    "$ H_2' $ is a sparse Hypothsis Set (有多項為零): ** HP-Hard to solve!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression with Softer Constraint\n",
    "\n",
    "為了避免 HP-Hard 問題，將限制變成 w平方的加總 小於某個常數 C。\n",
    "\n",
    "$$ \\sum_{q=0}^{10} w_q^2 \\le C $$\n",
    "\n",
    "Hypothesis Set 變為:\n",
    "\n",
    "$$ H(C) = \\Big \\{ w \\in \\Re^{10+1}, \\text{while } \\Vert w \\Vert^2 \\le C \\Big \\} $$\n",
    "\n",
    "H(C) 與 $ H_2 '$ overlap, 但不完全相同。  \n",
    "H(C) 不同的 C 有包含關係: $ H(0) \\subset H(1.126)  \\subset \\cdots  \\subset H(1126)  \\subset H(\\infty) $\n",
    "\n",
    "將被 C 限制大小的 w，定義為 $ w_{REG} $ : regularized hypothesis set H(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Form of Regularized Regression Problem\n",
    "\n",
    "$$ \n",
    "\\min_{w \\in \\Re^{Q+1}} \\ E_{in}(w) = \\frac{1}{N} \\sum_{n=1}^N \\big( w^T z_n - y_n \\big)^2 \n",
    " = \\frac{1}{N} \\sum_{n=1}^N \\big( Z \\vec{w} - \\vec{y} \\big)^T \\ \\big( Z \\vec{w} - \\vec{y} \\big)\n",
    "$$\n",
    "\n",
    "$$ s.t. \\underbrace{\\sum_{q=0}^Q w_q^2}_{w^T w} \\le C $$\n",
    "\n",
    "$ w^T w \\le C $ : feasible w within a radius $ \\sqrt{C} $ hypersphere.\n",
    "\n",
    "![img](imgs/c14-lagrange.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagrange Multiplier\n",
    "\n",
    "線性迴歸最佳解 $ w_{LIN} $ 是朝著梯度反方向 $ - \\nabla E_{in} $ 滾下\n",
    "\n",
    "Regularization 的限制是 w 只能在 球心原點半徑 $ \\sqrt{C} $ 的球範圍內。\n",
    "\n",
    "因此限制內的最佳解應該會出現在球的邊緣上。\n",
    "\n",
    "$ w^T w = C $ 球的法向量為 w (紅的normal),  \n",
    "因為不可以超出球的範圍，所以邊緣上的 w 只能再朝垂直於 w 的方向 $ w^{\\perp} $ 變化；  \n",
    "若是 $ w^{\\perp} $ 能在 $ -\\nabla E_{in} $ 方向找到分量 (綠分量)，  \n",
    "就能夠不違反 regularization 的限制，進一步優化 w，接近藍色谷底 $ w_{LIN} $\n",
    "\n",
    "基於這樣的概念，與 $ -\\nabla E_{in} $ 完全平行的 w 就是 Regularized Regression 的最佳解 $ w_{REG} $  \n",
    "因為 $ w_{REG}^{\\perp} $ 無法再找到 $ -\\nabla E_{in} $ 方向的分量，再進一步優化。\n",
    "\n",
    "Optimal Solution $ w_{REG} $:\n",
    "\n",
    "$$ -\\nabla E_{in}(w_{REG}) \\propto w_{REG} $$\n",
    "\n",
    "也就是要求解 Lagrange multiplier $ \\lambda \\gt 0 $ and $ w_{REG} $, such that\n",
    "\n",
    "$$ \\nabla E_{in} (w_{REG}) + \\frac{2 \\lambda}{N} w_{REG} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving Regularized w\n",
    "\n",
    "Solving\n",
    "\n",
    "$$ \\underbrace{\\ \\ \\nabla \\ E_{in} \\ (\\ w_{REG} \\ ) \\ \\ }_{\\frac{2}{N} \\big( Z^T Z \\vec{w} - Z^T \\vec{y} \\big) } + \\frac{2 \\lambda}{N} w_{REG} = 0 $$\n",
    "\n",
    "$$ \\frac{2}{N} \\Big( Z^T Z w_{REG} - Z^T y \\Big) + \\frac{2 \\lambda}{N} w_{REG} = 0 $$\n",
    "\n",
    "變化成了一個 $ w_{REG} $ 的線性方程式。\n",
    "\n",
    "$$ \\frac{2}{N} \\Big( Z^T Z w_{REG} - Z^T y + \\lambda w_{REG} \\Big) = 0 $$\n",
    "\n",
    "$$ Z^T Z w_{REG} + \\lambda w_{REG} =  Z^T y $$\n",
    "\n",
    "$$ \\big( Z^T Z + \\lambda I \\big) w_{REG} =  Z^T y $$\n",
    "\n",
    "$$ w_{REG} = \\big( Z^T Z + \\lambda I \\big)^{-1} Z^T y $$\n",
    "\n",
    "This is called Ridge Regression in Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented Error, Regularizer\n",
    "\n",
    "從另一個角度來看\n",
    "\n",
    "$$ \\nabla E_{in} (w_{REG}) + \\frac{2 \\lambda}{N} w_{REG} = 0 $$\n",
    "\n",
    "$ \\nabla E_{in} (w_{REG}) $ 是 $ E_{in} $ 的微分；上面的式子等於零，也就是說，想要上面式子的 **積分最小化**。\n",
    "\n",
    "Minimizing $ \\underbrace{E_{in}(w) + \\frac{\\lambda}{N} \\overbrace{w^T w}^{\\text{regularizer}}}_{\\text{augmented error} E_{aug}(w) } $\n",
    "\n",
    "Augmented Error 定義: $ E_{in}(w) + \\frac{\\lambda}{N} w^T w $\n",
    "\n",
    "Regularizer 定義: $ w^T w $\n",
    "\n",
    "將 難解的 Constrained $ E_{in} $ 轉化成 同等的 Augmented Error，如此可求得\n",
    "\n",
    "$$ w_{REG} \\leftarrow \\text{argmin}_w \\  E_{aug}(w) \\text{ for given } \\lambda \\ge 0 $$\n",
    "\n",
    "Minimizing unconstrained $ E_{aug} $ effectively minimizes some C-constrained $ E_{in} $\n",
    "\n",
    "使用 Regularizer 直接定 $ \\lambda $ 的值，就不需要再給 C 的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "![img](imgs/c14-result-reg.png)\n",
    "\n",
    "只要給一點的 $ \\lambda $ 就可以有效避免 overfitting.\n",
    "\n",
    "$ \\lambda $ 給太高則會出現 underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight-Decay Regularization:\n",
    "\n",
    "給大的 $ \\lambda $ 代表想要較短的 w, 較小的 C, 也就是 Constrain 的球較小。\n",
    "\n",
    "$ \\frac{\\lambda}{N} w^T w $ : called Weight-Decay regularization. (w)eight-衰退"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legendre Polynomials\n",
    "\n",
    "$$\n",
    "\\min_{w \\in \\Re^{Q+1}} \\frac{1}{N} \\sum_{n=0}^N \\big( w^T \\Phi(x_n) - y_n \\big)^2 + \\frac{\\lambda}{N} \\sum_{q=0}^Q w_q^2\n",
    "$$\n",
    "\n",
    "Naïve polynomial transform : $ \\Phi(x) = (1, x, x^2, \\cdots, x^Q) $\n",
    "\n",
    "在 naïve polynomial transform 中，當 $ x_n \\in \\big[ -1, +1 \\big] $ 高次項 $ x_n^q $ 需要很大的 $ w_q $ 才會起作用。\n",
    "\n",
    "因此使用 Orthonormal Basis Functions : Legendre Polynomials\n",
    "\n",
    "定義 Dot Product $ \\langle f, g \\rangle = \\int_{-1}^{1} \\big[ \\ f(x) \\ g(x) \\ \\big] d_x $, 任意兩個函數內積 $ L_i \\cdot L_j = 0 $，  \n",
    "產生 Legendre Polynomials\n",
    "\n",
    "Normalized Polynomial transform: $ \\Phi'(x) = \\big( 1, L_1(x), L_2(x), \\cdots, L_Q(x) \\big) $\n",
    "\n",
    "![img](imgs/c14-legendre-xs.png)\n",
    "\n",
    "![img](imgs/c14-legendre-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization and VC Theory\n",
    "\n",
    "VC Guarantee of Constrained-Minimizing $ E_{in} $, VC Bound:\n",
    "\n",
    "$$ E_{out}(w) \\le E_{in}(w) + \\Omega\\big( H(C) \\big) $$\n",
    "\n",
    "Augmented Error\n",
    "\n",
    "$$ E_{aug}(w) = E_{in}(w) + \\frac{\\lambda}{N} w^T w $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regularizer $ w^T w = \\Omega \\big( w \\big) $ : 可以看做 Single Hypothesis 的複雜度 complexity, ex:越高次越複雜\n",
    "\n",
    "Generalization Price $ \\Omega \\big( H \\big) $ : 整個 Hypothesis Set 的複雜度。\n",
    "\n",
    "Note: $ \\Omega $ 代表複雜度。\n",
    "\n",
    "如果 $ \\frac{\\lambda}{N} \\Omega(w) $ 可代表 $ \\Omega(H) $，\n",
    "\n",
    "$ E_{aug} $ is a better proxy of $ E_{out} $ than $ E_{in} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effective VC Dimension\n",
    "\n",
    "### $$ \\min_{w \\in \\Re^{\\tilde{d}+1}} E_{aug} (w) = E_{in} (w) + \\frac{\\lambda}{N} \\Omega (w) $$\n",
    "\n",
    "Model Complexity $ d_{VC} (H) = \\tilde{d} + 1 $ : 這是所有 {w} 項都在最小化過程中被考慮的複雜度，對應的 VC Dimension\n",
    "\n",
    "而加上 C (或是 $ \\lambda $) 的 Regularization 限制後，不是所有 {w} 項都被考慮，  \n",
    "而是在 C 範圍內，接近 H 谷底附近出現的 w 才會被考慮。被考慮的 Hypothesis Set 是 H(C)。\n",
    "\n",
    "如此衍生出 **Effective VC Dimension** 的概念，是對應到被 C 限制 Hypothesis Set : H(C) 的 VC Dimension:\n",
    "\n",
    "$$ d_{EFF} \\big( H, \\underbrace{\\ \\ A \\ \\ }_{\\min E_{aug}} \\big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Regularizers $ \\Omega(w) $\n",
    "\n",
    "朝著 target function f 的方向去定 Constraint\n",
    "\n",
    "#### target-dependent: some properties of target, if known\n",
    "\n",
    "- symmetry regularizer: $ \\sum \\big[ \\text{q is odd} \\big]_{bool} w_q^2 $\n",
    "\n",
    "#### plausible: direction towards smoother or simpler\n",
    "\n",
    "- stochastic / deterministic noise both are non-smooth. sparsity (L1) regularizer: $ \\sum \\vert{w_q}\\vert $\n",
    "\n",
    "#### friendly: easy to optimize\n",
    "\n",
    "- weight-decay (L2) regularizer: $ \\sum w_q^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularizer 的選擇，與 Error Measure 的選擇相近\n",
    "\n",
    "| Regularizer | Error Measure |\n",
    "|-|-|\n",
    "| Target-Dependent | User-Dependent |\n",
    "| Plausible | Plausible |\n",
    "| Friendly | Friendly |\n",
    "\n",
    "#### Augmented Error = Error $ \\hat{err} $ + regularizer $ \\Omega $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 and L1 Regularizer\n",
    "\n",
    "L2 Regularizer: $ \\Omega(w) = \\sum_{q=0}^Q w_q^2 = \\Vert w \\Vert^2_2 $\n",
    "\n",
    "- 平滑，可微分 Convex, Differentiable\n",
    "- 容易做最佳化 Easy to optimize\n",
    "\n",
    "L1 Regularizer: $ \\Omega(w) = \\sum_{q=0}^Q \\big| w_q \\big| = \\Vert w \\Vert_1 $,  \n",
    "是一個尖的多邊體，每個尖的頂點都代表有些分量 $ w_q$ 為 0 (sparsity)\n",
    "\n",
    "- Convex, Not differentiable everywhere\n",
    "- Sparsity in solution, 解容易出現在頂點上。\n",
    "\n",
    "因為 L1 Regularizer 是 sparse solution, 在獲得 solution (很多$ w_q=0 $) 進行 prediction 時侯較快。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](imgs/c14-l1l2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal $ \\lambda $\n",
    "\n",
    "![img](imgs/c14-optimal-lambda.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "右圖用一個 15次 多項式 進行，藍線是 用 $ H_{15} $ 逼近原來的 target function $ f_{15} $\n",
    "\n",
    "more noise needs more regularization $ \\lambda $\n",
    "\n",
    "但實際上，不會知道 noise 是多少，所以需要在多個不同的 $ \\lambda $ 中去選擇出一個較好的。"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
