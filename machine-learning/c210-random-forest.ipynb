{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "結合 Bagging &amp; Decision Tree 的特性，做 aggregation of aggregation.\n",
    "\n",
    "Bagging (Random): reduce variance by voting / average\n",
    "\n",
    "- for t=1,2,...,T; 利用 bootstrap, 從 D 中取出 N' 筆資料當成 $ \\tilde{D_t} $\n",
    "- 把 $ \\tilde{D_t} $ 給 base algorithm: A, 獲得 $ g_t $\n",
    "- 讓每一個 $ g_t $ 投票獲得 G = $ Uniform( \\{ g_t \\} ) $\n",
    "\n",
    "Decition Tree (Forest): large variance, especially if fully-grown\n",
    "\n",
    "- learn b(x) and split D to $ D_c $ by b(x)\n",
    "- build $ G_c \\leftarrow \\ DTree( D_c ) $\n",
    "- return $ G(x) = \\sum_{c=1}^C \\big[ b(x) = c \\big]_{boolean} G_c(x) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest = Bagging + Full-grown C&amp;RT decision tree\n",
    "\n",
    "- for t=1,2,...,T; 利用 bootstrap, 從 D 中取出 N' 筆資料當成 $ \\tilde{D_t} $\n",
    "- 把 $ \\tilde{D_t} $ 給 base algorithm: $ DTree(\\tilde{D_t}) $, 獲得 $ g_t $\n",
    "- 讓每一個 $ g_t $ 投票獲得 G = $ Uniform( \\{ g_t \\} ) $\n",
    "\n",
    "Random Forest 的優點:\n",
    "\n",
    "- highly parallel / efficient to learn\n",
    "- inherit pros of C&amp;RT \n",
    "- eliminate cons of fully-grown tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversifying by Feature Projection\n",
    "\n",
    "原來 bagging 的 diversity 來自於 randomly sample N' examples from D\n",
    "\n",
    "另一個可能性是 : randomly sample d' features from x.\n",
    "\n",
    "$$\n",
    "\\mathcal{Z} \\in \\Re^{d'} : \\text{a Random Subspace of } \\mathcal{X}\n",
    "$$\n",
    "\n",
    "re-sample new subspace for each b(x) in C&amp;RT\n",
    "\n",
    "RF = bagging + random subspace C&amp;RT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversifying by Feature Expansion\n",
    "\n",
    "上述是將 feature 投影到 natural basis 上 (random subspace)\n",
    "\n",
    "進一步更可以投影在隨機的 basis 上，就如同將幾個 feature 做了 combination \n",
    "\n",
    "RF = bagging + random combination C&amp;RT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-Of-Bag estimation\n",
    "\n",
    "若資料有 N 筆，做 bootstrapping N' = N, 某一筆資料 $ (x_n,y_n) $ 不在 (out of bag) 用來產生 $ g_t $ 的機率是:\n",
    "\n",
    "$$\n",
    "\\big( 1 - \\frac{1}{N} \\big)^N \\\\\n",
    "= \\frac{1}{ \\big( 1 + \\frac{1}{N-1} \\big)^N } \\approx \\frac{1}{e}\n",
    "$$\n",
    "\n",
    "所以對每一個 $ g_t $ 來說，大概有 0.368 N (1/e) 筆的資料是 OOB, 不在用來產生該 hypothesis 的資料中。\n",
    "\n",
    "可以使用 OOB 資料來 validate 該 $ g_t $，但很少需要這樣做。\n",
    "\n",
    "通常會用 OOB 資料來驗證 G, 做法如下:\n",
    "\n",
    "- 由數個 $ g_i $ 產生 $ G_n^- $, 對這幾個 $ g_i $ 的 OOB 就可以拿來 validate $ G_n^- $\n",
    "- 然後將 OOB 得到的 Error 做平均求得 $ E_{OOB} $\n",
    "\n",
    "$$\n",
    "E_{OOB}(G) = \\frac{1}{N} \\sum_{n=1}^N err \\big( y_n, G_n^-(x_n) \\big)\n",
    "$$\n",
    "\n",
    "$ E_{OOB} $ : self validation of bagging / RF\n",
    "\n",
    "也不需要再做 re-training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "當有許多的 feature, 其中可能有些是與要做的 prediction 無關的。如何 select 出有用的 feature.\n",
    "\n",
    "例如 redundant, irrelevant features.\n",
    "\n",
    "Decision Tree: a rare model with build-in Feature Selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection by Importance\n",
    "\n",
    "importance by linear model\n",
    "\n",
    "$$\n",
    "score = w^T x = \\sum_{i=1}^d w_i x_i\n",
    "$$\n",
    "\n",
    "- intuitive estimate: importance(i) = $ | w_i | $, with some good w\n",
    "- getting good w: learned from data (linear model)\n",
    "- non-linear models? often much harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance by Permutation Test\n",
    "\n",
    "如果 feature $ x_i $ 是重要的，放些 random values 到 $ x_i $ 中，會 degrade performance.\n",
    "\n",
    "用什麼 random values ?\n",
    "\n",
    "- 如果用某種特定分佈 (Uniform, Gaussian...) 原來的 資料分佈 $ P(x_i) $ 會被影響。\n",
    "- 用 bootstrap, permutation , 原來的 資料分佈 $ P(x_i) $ 可以大致上保留下來。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Test\n",
    "\n",
    "importance(i) = performance(D) - performance( $ D^p $ )\n",
    "\n",
    "$ D^p $ 是將第 i 個 feature : $ x_i $, 中的資料順序打亂，其他 feature 資料不變。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Test in RF\n",
    "\n",
    "importance(i) = performance(D) - performance( $ D^p $ )\n",
    "\n",
    "importance(i) = $ E_{OOB}(G) - E_{OOB}(G^p) $\n",
    "\n",
    "要獲得 $ G^p $ 比較麻煩，可以照樣用 G，但是用 $ E_{OOB}^p $ (在 OOB 驗證 G 時候，在 feature $ x_i $ 上進行 permutation )\n",
    "\n",
    "因此變成\n",
    "\n",
    "importance(i) = $ E_{OOB}(G) - E_{OOB}^p(G) $\n",
    "\n",
    "實務中，用 RF + permutation + OOB 來做 feature selection 是 efficient + promising."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
