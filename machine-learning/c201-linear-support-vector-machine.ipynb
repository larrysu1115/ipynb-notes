{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](imgs/c201-a-svm.png)\n",
    "\n",
    "```\n",
    "if Gaussian-like noise on future x:\n",
    "\n",
    "Xn further from hyperplane             distance to closest Xn\n",
    "tolerate more noise           <===>    amount of noise tolerance\n",
    "more robust to overfitting    <===>    robustness of hyperplane\n",
    "```\n",
    "\n",
    "robustness = fatness: distance to closest $ x_n $\n",
    "\n",
    "#### Goal: find fattest separating hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "margin: fatness\n",
    "\n",
    "correctness: $ y = sign \\big( w^T x_n \\big) $\n",
    "\n",
    "$$ \\max_w \\ \\ margin \\big( w \\big) $$\n",
    "\n",
    "$$ y_n w^T x_n \\gt 0 $$\n",
    "\n",
    "$$ margin \\big( w \\big) = \\min_{n=1,\\cdots,N} distance \\big( x_n, w \\big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面章節用的 w 定義為 $ \\big( w_0 = b \\big), w_1, w_2, \\cdots, w_d $,  \n",
    "與這裏的 w 定義略有不同，把 $ w_0 $ 區分出來記為 b，\n",
    "\n",
    "$ h(x) = sign \\big( w^T x + b \\big) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance to Hyperplane\n",
    "\n",
    "want: distance(x, b, w) with hyperplane $ w^T x' + b = 0 $\n",
    "\n",
    "![img](imgs/c201-b-distance.png)\n",
    "\n",
    "w 是平面的法向量，考慮 x' , x'' 兩個點，  \n",
    "$ w^T x' = -b $,  \n",
    "$ w^T x'' = -b $\n",
    "\n",
    "$ w \\perp $ hyperplane, 因為 向量 (x'' - x') 與 $ \\vec{w} $ 是 orthogonal:  \n",
    "$ \\Big( w^T \\underbrace{\\big( x'' - x' \\big)}_{\\text{vector on hyperplane}} \\Big) = 0 $\n",
    "\n",
    "distance = 投射 ( x - x' ) 到 $ \\vec{w} $ 的長度\n",
    "\n",
    "$ distance(x, b, w) = \\Big| \\frac{w^T}{\\Vert w \\Vert} \\big( x - x' \\big) \\Big| = \\frac{1}{\\Vert w \\Vert} \\Big|  w^T x + b \\Big| $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance to Separating Hyperplane\n",
    "\n",
    "Separating hyperplane: for every n\n",
    "\n",
    "$$ y_n \\big( w^T x_n + b \\big) \\gt 0 $$\n",
    "\n",
    "distance of $ x_n $ to separating hyperplane ( y is either +1 or -1 ):\n",
    "\n",
    "$$ distance(x_n, b, w) = \\frac{1}{\\Vert w \\Vert} y_n \\Big| \\  w^T x_n + b \\ \\Big| $$\n",
    "\n",
    "求解的式子轉變成:\n",
    "\n",
    "$$ \\max_{b,w} \\ \\ margin \\big( b, w \\big) $$\n",
    "\n",
    "$$ y_n ( w^T x_n + b ) \\gt 0 $$\n",
    "\n",
    "拿掉絕對值:\n",
    "\n",
    "$$ margin \\big( b, w \\big) = \\min_{n=1,\\cdots,N} \\frac{1}{\\Vert w \\Vert} y_n \\Big( \\  w^T x_n + b \\ \\Big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Margin of Special Separating Hyperplane\n",
    "\n",
    "將線的方程式 scale 縮放 後的結果是不變的。\n",
    "\n",
    "$ w^T x + b = 0 \\iff 3 w^T x + 3 b = 0 $\n",
    "\n",
    "minimum margin 也可以縮放到剛好為 1，本質是不變的。\n",
    "\n",
    "因此讓 $ x_1, x_2, \\cdots , x_N $ 中，使得 \n",
    "$ y_n \\big( w^T x_n + b \\big) $ \n",
    "最小的那個 $ (x_i, y_i) $ 套入式子中得到的長度是 1\n",
    "\n",
    "$ y_i \\big( w^T x_i + b \\big) = 1 $\n",
    "\n",
    "$$ margin \\big( b, w \\big) = \\min_{n=1,\\cdots,N} \\frac{1}{\\Vert w \\Vert} y_n \\Big| \\  w^T x_n + b \\ \\Big| = \\frac{1}{\\Vert w \\Vert} $$\n",
    "\n",
    "求解的式子進一步轉變成:\n",
    "\n",
    "$$ \\max_{b,w} \\ \\ \\frac{1}{\\Vert w \\Vert} $$\n",
    "\n",
    "$$ y_n ( w^T x_n + b ) \\gt 0 $$\n",
    "\n",
    "條件: $ \\min_{n=1,\\cdots,N} y_n \\big( w^T x_n + b \\big) = 1 $, 這個條件已經隱含了上面大於零的意義，  \n",
    "因此有這個條件就可以拿掉上面大於零的式子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Large-Margin Hyperplane Problem\n",
    "\n",
    "$$ \\max \\ \\ margin(b,w) = \\max_{b,w} \\frac{1}{\\Vert w \\Vert} $$\n",
    "\n",
    "$$ \\text{,subject to: } \\min_{n=1,\\cdots,N} y_n \\big( w^T x_n + b \\big) = 1 $$\n",
    "\n",
    "找出最小值等於一的條件不好處理，於是希望將這個條件放寬成 $ \\le 1 $,   \n",
    "證明這樣的放寬，仍然會找到最佳解落在原來的條件限制中:\n",
    "\n",
    "如果\"最佳解 (b,w) 落在 原始條件範圍外\"， $ y_n ( w^T x_n + b ) \\ge 1.126 $ for all n,  \n",
    "仍然是可以縮放該式子得到 \"更優化的解\" : $ ( b/1.126, w/1.126 ) $, 那麼原來的 (b,w) 就不是最佳解了！Contradiction!  \n",
    "所以 \"最佳解 (b,w) 落在 原始條件範圍外\" 為假，最佳解仍然會落在原始條件範圍內。\n",
    "\n",
    "最後，再將最大化的部分做倒數，變成最小化問題: \n",
    "\n",
    "$ \\max_{b,w} \\frac{1}{\\Vert w \\Vert} \\to \\min_{b,w} \\Vert w \\Vert \\to \\min_{b,w} \\sqrt{w^T w} \\to \\min_{b,w} w^T w \\to \\min_{b,w} \\frac{1}{2} w^T w $\n",
    "\n",
    "準備好最後要處理的問題是:\n",
    "\n",
    "$$ \\min_{b,w} \\frac{1}{2} w^T w $$\n",
    "\n",
    "$$ y_n \\big( w^T x_n + b \\big) \\ge 1 \\text{ for all n.} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Programming 二次規劃\n",
    "\n",
    "上述的\n",
    "\n",
    "1. 求二次方程式最小值\n",
    "2. 附帶 一次方程式 的限制\n",
    "\n",
    "是標準的 QP (Quadratic Programming) 問題：\n",
    "\n",
    "Optimal: $ u \\leftarrow QP(Q, \\vec{p}, A, \\vec{c}) $\n",
    "\n",
    "Minimal: $ \\min_u \\frac{1}{2} u^T Q u + p^T u $\n",
    "\n",
    "Subject to: $ a_m^T \\ge C_m $, for m = 1,2,...,M\n",
    "\n",
    "u: u向量，  \n",
    "要求解 u 向量的二次函數；  \n",
    "其二次的係數放在 Q 矩陣中，  \n",
    "其一次項的係數放在 p 向量中。  \n",
    "條件的部份，是 u向量的一次函數，係數放在 向量 $ a_m $  中，  \n",
    "條件要大於的值放在 $ c_m $ 中, 有 M 個條件。  \n",
    "將 向量$a_m$ 集合起來放在 A矩陣 中， $ c_m $集合起來放在 c向量 中。\n",
    "\n",
    "將上面要求解的 SVM 問題代入:\n",
    "\n",
    "$ u = \\begin{bmatrix} b \\\\ w \\end{bmatrix} $\n",
    "\n",
    "$ Q = \\begin{bmatrix} 0 & 0_d^T \\\\ 0_d & I_d \\end{bmatrix} $\n",
    "\n",
    "$ p = 0_{d+1} $\n",
    "\n",
    "$ a_n^T = y_n \\ \\begin{bmatrix} 1 & x_n^T \\end{bmatrix} $\n",
    "\n",
    "$ c_n = 1, \\  M = N $\n",
    "\n",
    "$$ \\begin{bmatrix} b \\\\ w \\end{bmatrix} \\leftarrow QP( Q, \\vec{p}, A, \\vec{c} ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "以四個點來看\n",
    "```\n",
    "x1 = (0,0), y1 = -1\n",
    "x2 = (2,2), y1 = -1\n",
    "x3 = (2,0), y1 = +1\n",
    "x4 = (3,0), y1 = +1\n",
    "```\n",
    "\n",
    "$ a_1^T = [-1, 0, 0] $  \n",
    "$ a_2^T = [-1,-2,-2] $  \n",
    "$ a_3^T = [ 1, 2, 0] $  \n",
    "$ a_4^T = [ 1, 3, 0] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Hard-Margin SVM Algorithm\n",
    "\n",
    "Linear: 使用原來的 x，直線\n",
    "\n",
    "Hard-Margin: 必須嚴格分開 + / -\n",
    "\n",
    "want non-linear? $ z_n = \\Phi \\big( x_n \\big) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Large-Margin Hyperplane ?\n",
    "\n",
    "$$ \\min_{b,w} \\frac{1}{2} w^T w $$\n",
    "\n",
    "$$ y_n \\big( w^T z_n + b \\big) \\ge 1 \\text{ for all n.} $$\n",
    "\n",
    "note: x 經過 $ \\Phi $ transform 到 z 空間\n",
    "\n",
    "比較 Regularization 與 SVM 算法中，兩者做的事情是一體兩面，所求的 minimize 與 constraint 互換。  \n",
    "SVM 可以看作是一種 Regularization, 著重在可以抵抗一些測量誤差上。\n",
    "\n",
    "| |minimize|constraint|\n",
    "|-|-|-|\n",
    "| regularization | $ E_{in} $ | $ w^T w \\le C $ |\n",
    "| SVM | $ w^T w $ | $ E_{in} = 0 $ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large-Margin Restricts Dichotomies\n",
    "\n",
    "Consider \"Large-Margin\" algorithm $ A_{\\rho} $ :\n",
    "\n",
    "- returns g with margin(g) $ \\ge \\rho $\n",
    "- otherwise return 0 if cannot find margin large enough.\n",
    "\n",
    "三個點在 $ \\mathcal{R}^2 $ 空間，沒有限制去做 shatter, 可有 $ 2^3 $ 八種方式 dichotomies。\n",
    "\n",
    "但如果限定 margin 最少要有 p 的大小，就不一定做得出八種方式。\n",
    "\n",
    "Fewer dichotomies --> smaller VC dimension --> better generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VC Dimension of Large-Margin Algorithm\n",
    "\n",
    "$$ d_{VC} \\big( A_{\\rho} \\big) \\text{ when } X = \\text{ unit circle } in \\mathcal{R}^2 $$\n",
    "\n",
    "若是資料 x 都在單位圓(半徑=1)範圍內，  \n",
    "如果限制 min margin $ \\rho \\gt \\frac{\\sqrt{3}}{2} $  \n",
    "則任何三個點都無法 shatter, $ d_{VC} \\lt 3 $\n",
    "\n",
    "一般來說，當 x in radius-R hyperball:\n",
    "\n",
    "$$ d_{VC} \\big( A_{\\rho} \\big) \\le \\min \\Big( \\frac{R^2}{\\rho^2} \\Big) + 1 \\le \\underbrace{d + 1}_{ d_{VC}(\\text{ perceptrons }) } $$\n",
    "\n",
    "> 可以利用 $ \\rho $ 降低 VC dimension, 控制算法的複雜度。\n",
    "\n",
    "原本的空間轉換 Feature Transform 可以帶來複雜的 boundary, 卻發生增加了許多 hypothesis 的問題，  \n",
    "可以結合 SVM 的 margin 控制，不要增加太多 hypothesis, 但還是獲得複雜的 boundary。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
