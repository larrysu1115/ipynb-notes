{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on any (b,w), $ \\xi_n $ = margin violation = $ \\max \\big( 1 - y_n (w^T z_n + b), 0 \\big) $\n",
    "\n",
    "- DO Violating Margin: $ \\xi_n = 1 - y_n (w^T z_n + b) $\n",
    "- Not Violating Margin: $ \\xi_n = 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原來的 soft margin SVM prime 問題:\n",
    "\n",
    "$\n",
    "\\min_{b,w,\\xi} \\frac{1}{2} w^T w + C \\cdot \\sum_{n=1}^N \\xi_n \\\\\n",
    "s.t. \\ \\ y_n \\big( w^T z_n + b \\big) \\ge 1 - \\xi_n \\text{ and } \\xi_n \\ge 0 \\text{ for all n }\n",
    "$\n",
    "\n",
    "經過代入 $ \\xi_n $ 變成:\n",
    "\n",
    "$\n",
    "\\min_{b,w} \\ \\ \\Big( \\frac{1}{2} w^T w + C \\cdot \\sum_{n=1}^N \\max \\big( 1 - y_n (w^T z_n + b), 0 \\big) \\Big)\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "最小化錯誤，並加上限制 w 的長度。\n",
    "\n",
    "$ \\min \\ \\ \\frac{1}{2} w^T w + C \\sum \\widehat{err} $\n",
    "\n",
    "Just L2 Regularization:\n",
    "\n",
    "$ \\min \\ \\ \\frac{\\lambda}{N} w^T w + \\frac{1}{N} \\sum err $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|                            | minimize          | constraint       |\n",
    "|-|-|-|\n",
    "|Regularization by constraint| $ E_{in} $ .......................... | $ w^T w \\le C $ |           \n",
    "|Hard-Margin SVM             | $ w^T w $ ........................... | $ E_{in} = 0 $, and more... |\n",
    "|L2 Regularization           | $ \\frac{\\lambda}{N} w^T w + E_{in} $  .................................. | |\n",
    "|Soft-Margin SVM             | $ \\frac{1}{2} w^T w + C_{soft} N \\widehat{E_{in}} $ | ... |\n",
    "\n",
    "Larger C = smaller $ \\lambda $ = less regularization\n",
    "\n",
    "將 SVM 看做一種 regularization model，就可以再延伸這個概念到其他的 learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithmic Error Measure of SVM\n",
    "\n",
    "<img src=\"imgs/c205-errx3.png\" style=\"float:right;\" />\n",
    "\n",
    "令 linear score $ s = w^T z_n + b $\n",
    "\n",
    "$ err_{0/1} (s,y) = \\big[ y s \\le 0 \\big]_{boolean} $\n",
    "\n",
    "$ \\widehat{err}_{SVM} (s,y) = max\\big( 1 - ys, 0 \\big) $ : upper bound of $ err_{0/1} $  \n",
    "also called **hinge error measure**\n",
    "\n",
    "$ err_{SCE} (s,y) = \\log_2 \\big( 1 + \\exp(-ys) \\big) $ : upper bound of $ err_{0/1} $  \n",
    "used in **logistic regression**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| $ -\\infty $ | --------------------- $ ys $ --------------------- | $ +\\infty $ |\n",
    "|-|-|-|\n",
    "| $ \\approx -ys $ | $ \\widehat{err_{SVM}}(s,y) $ | = 0 |\n",
    "| $ \\approx -ys $ | $ \\log_2 \\cdot err_{SCE}(s,y) $ | $ \\approx 0 $ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "從這樣角度看， Regularized Logistic Regression 幾乎就等同於 Soft Margin SVM\n",
    "\n",
    "反過來說，Soft Margin SVM 是否也就等同 Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM for Soft Binary Classification\n",
    "\n",
    "#### Naïve Idea 1:\n",
    "\n",
    "1. run SVM and get $ (b_{SVM}, w_{SVM}) $\n",
    "\n",
    "2. return $ g(x) = \\theta ( w_{SVM}^T x + b_{SVM} ) $\n",
    "\n",
    "direct use of similarity, works reasonably well.\n",
    "\n",
    "no LogReg flavor.\n",
    "\n",
    "#### Naïve Idea 2:\n",
    "\n",
    "1. run SVM and get $ (b_{SVM}, w_{SVM}) $\n",
    "\n",
    "2. run LogReg with $ (b_{SVM}, w_{SVM}) $ as $ w_0 $ 起始點\n",
    "\n",
    "2. return LogReg solution as $ g(x) $\n",
    "\n",
    "Not really 'easier' than original LogReg.\n",
    "\n",
    "SVM flavor (kernel) lost?\n",
    "\n",
    "可否融合以上兩種方式，獲得兩邊的好處 flavors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-Level Learning\n",
    "\n",
    "$$\n",
    "g(x) = \\theta \\Big( A \\cdot \\big( w_{SVM}^T \\Phi(x) + b_{SVM} \\big) + B \\Big)\n",
    "$$\n",
    "\n",
    "- SVM flavor: fix hyperplane direction by $ w_{SVM} $ - kernel applies.\n",
    "- LogReg flavor: fine-tune hyperplane to match maximum likelihood by scaling (A) and shifting (B).\n",
    "\n",
    "- often $ A \\gt 0 $ if $ w_{SVM} $ reasonably good.\n",
    "- often $ B \\approx 0 $ if $ b_{SVM} $ reasonably good.\n",
    "\n",
    "### new LogReg Problem:\n",
    "\n",
    "$$\n",
    "\\min_{A,B} \\frac{1}{N} \\sum_{n=1}^{N} \\log \\Big( \\ \\ 1 + \\exp \\Big( -y_n \\big( A \\cdot \\underbrace{( w_{SVM}^T \\Phi(x_n) + b_{SVM} )}_{\\Phi_{SVM}(x_n)} + B \\big) \\Big) \\ \\ \\Big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic SVM - Platt's Model of Probabilistic SVM for Soft Binary Classification\n",
    "\n",
    "STEP 1: run SVM on D to get $ (b_{SVM}, w_{SVM}) $ or the equivalent $ \\alpha $,  \n",
    "and transform D to $ z_n' = w_{SVM}^T \\Phi(x_n) + b_{SVM} $\n",
    "\n",
    "STEP 2: run LogReg on $ \\big\\{ \\big( z_n', y_n \\big) \\big\\}_{n=1}^N $ to get (A,B)\n",
    "\n",
    "STEP 3: return $ g(x) = \\theta \\Big( A \\cdot \\big( w_{SVM}^T \\Phi(x) + b_{SVM} \\big) + B \\Big) $\n",
    "\n",
    "NOTE: 實際的 Platt's Model 要更複雜些。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Soft Binary Classifier not having the same boundary as SVM classifier, because of B.\n",
    "- how to solve LogReg: GD / SGD / or other (only 2 variables)\n",
    "\n",
    "kernel SVM = approx. LogReg in Z-space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal w\\* be represented by $ z_n $ ?\n",
    "\n",
    "## Representer Theorem\n",
    "\n",
    "claim: for any L2-regularized linear model\n",
    "\n",
    "$$\n",
    "\\min_w \\frac{\\lambda}{N} w^T w + \\frac{1}{N} \\sum_{n=1}^N err \\big( y_n, w^T z_n \\big)\n",
    "$$\n",
    "\n",
    "optimal $ w_{*} = \\sum_{n=1}^N \\beta_n z_n $\n",
    "\n",
    "#### Any L2-regularized linear model can be kernelized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Logistic Regression\n",
    "\n",
    "solving L2-regularized logistic regression\n",
    "\n",
    "$$\n",
    "\\min_w \\ \\ \\frac{\\lambda}{N} w^T w + \\frac{1}{N} \\sum_{n=1}^N \\log \\Big( 1 + \\exp \\big( - y_n w^T z_n \\big) \\Big)\n",
    "$$\n",
    "\n",
    "yields optimal solution \n",
    "\n",
    "$$\n",
    "w_* = \\sum_{n=1}^N \\beta_n z_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without loss of generality, can solve for optimal $ \\beta $ instead of w。將 w 部份用 $ \\beta,z $ 取代:\n",
    "\n",
    "$$\n",
    "\\min_w \\ \\ \\frac{\\lambda}{N} \\sum_{n=1}^N \\sum_{m=1}^N \\beta_n \\beta_m K(x_n, x_m) + \\frac{1}{N} \\sum_{n=1}^N \\log \\Big( 1 + \\exp \\big( - y_n \\sum_{m=1}^N \\beta_m K(x_m, x_n) \\big) \\Big)\n",
    "$$\n",
    "\n",
    "然後用 GD / SGD / ... for unconstrained optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Logistic Regression (KLR) : Another View\n",
    "\n",
    "$ \\sum_{m=1}^N \\beta_m K(x_m, x_n) $ : inner product between $ \\beta $ and transformed data\n",
    "$ \\big( K(x_1, x_n), K(x_2, x_n), \\cdots , K(x_N, x_n) \\big) $\n",
    "\n",
    "$ \\sum_{n=1}^N \\sum_{m=1}^N \\beta_n \\beta_m K(x_n, x_m) $ : a special regularizer $ \\beta^T K \\beta $\n",
    "\n",
    "KLR = linear model of $ \\beta $ with kernel as transform and kernel regularizer.\n",
    "\n",
    "KLR = linear model of w with embedded-in-kernel transform and L2 regularizer\n",
    "\n",
    "WARNING: coefficients $ \\beta_n $ in KLR often non-zero."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
