{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如有一組顧客資料，有如下 features:\n",
    "\n",
    "```\n",
    "|----|----feature---|-------data---------|\n",
    "| x1 |          age |         23   years |\n",
    "| x2 |       salary |  1,000,000   NTD   |\n",
    "| x3 | years in job |          0.5 years |\n",
    "| x4 |         debt |    200,000   NTD   |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features of customer: $ \\vec{x} = \\big( x_0, x_1, x_2, \\cdots, x_d \\big) $\n",
    "\n",
    "$ x_0 $ 為自行加上的常數項。\n",
    "\n",
    "給每個 feature 加上權重  w, feature乘上各自的權重後相加，獲得 貸款額度 y。\n",
    "\n",
    "$$\n",
    "y \\approx \\sum_{i=0}^{d} w_i x_i\n",
    "$$\n",
    "\n",
    "Linear regression hypothesis: $ h(x) = w^T x $\n",
    "\n",
    "Linear Regression: Find lines(x:1D) / hyperplanes(x:2D) with small residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Measure\n",
    "\n",
    "Squared Error: $ \\text{err }( \\hat{y}, y) = ( \\hat{y} - y)^2 $\n",
    "\n",
    "$ \n",
    "\\begin{align}\n",
    "E_{in}(w) & = \\frac{1}{N} \\sum_{n=1}^N \\big( h(x_n) - y_n \\big)^2 \\\\\n",
    "          & = \\frac{1}{N} \\sum_{n=1}^N \\big( w^T x_n - y_n \\big)^2\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "$ \n",
    "E_{out}(w) = \\mathcal{E}_{x,y \\sim P} \\sum_{n=1}^N \\big( w^T x - y_n \\big)^2\n",
    "$\n",
    "\n",
    "How to minimize $ E_{in} $ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \n",
    "E_{in}(w) = \\frac{1}{N} \\sum_{n=1}^N \\big( w^T x_n - y_n \\big)^2 = \\frac{1}{N} \\sum_{n=1}^N \\big( x_n^T w - y_n \\big)^2\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "= \\frac{1}{N} \\begin{Vmatrix} \n",
    "\\vec{x}_1^T \\vec{w} - y_1 \\\\\n",
    "\\vec{x}_2^T \\vec{w} - y_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\vec{x}_N^T \\vec{w} - y_N\n",
    "\\end{Vmatrix}^2\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ = \\frac{1}{N} \n",
    "\\Big\\Vert\n",
    "\\begin{bmatrix}\n",
    "\\vec{x}_1^T \\\\ \\vec{x}_2^T \\\\ \\vdots \\\\ \\vec{x}_N^T\n",
    "\\end{bmatrix} \\vec{w}\n",
    "- \\begin{bmatrix}\n",
    "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N\n",
    "\\end{bmatrix}\n",
    "\\Big\\Vert^2 $, 把每一個向量 $ \\vec{x} $ 組成 [矩陣X], 每一個 y 組成向量 $ \\vec{y} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ E_{in}(w) = \\frac{1}{N} \n",
    "\\Big\\Vert\n",
    "X \\vec{w} - \\vec{y}\n",
    "\\Big\\Vert^2 $\n",
    "\n",
    "$ E_{in}(w) $ : Continuous 連續, Differentiable 可微分 , Convex 凸函數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最小值會出現在梯度 $ \\nabla E_{in}(w) $ 為 零 的地方，\n",
    "\n",
    "意即最好的 w 出現在:\n",
    "\n",
    "$$\n",
    "\\nabla E_{in}(w) \\equiv \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial}{\\partial w_0} E_{in}(w) = 0 \\\\\n",
    "\\frac{\\partial}{\\partial w_1} E_{in}(w) = 0 \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial}{\\partial w_d} E_{in}(w) = 0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以目的就是求得 Linear Reg. 的最佳解 $ w_{LIN} $ such that $ \\nabla E_{in}(w_{LIN}) = 0 $\n",
    "\n",
    "$\n",
    "E_{in}(w) = \\frac{1}{N} \\Big\\Vert X w - y \\Big\\Vert^2 = \\frac{1}{N} \\Big( w^T X^T X w - 2 w^T X^T y + y^T y \\Big)\n",
    "$\n",
    "\n",
    "### The Gradient $ \\nabla E_{in}(w) $\n",
    "\n",
    "Let:  \n",
    "$ X^T X = A $ 矩陣  \n",
    "$ X^T y = \\vec{b} $ 向量  \n",
    "$ y^T y = c $ 常數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ E_{in}(w) = \\frac{1}{N} \\Big( w^T A w - 2 w^T \\vec{b} + c \\Big) $\n",
    "\n",
    "$ \\nabla E_{in}(w) = \\frac{1}{N} \\Big( 2 A w - 2 \\vec{b} \\Big) = \\frac{2}{N} \\Big( X^T X w - X^T y \\Big) = 0 $\n",
    "\n",
    "$ w_{LIN} = \\Big( X^T X \\Big)^{-1} X^T y $\n",
    "\n",
    "Pseudo-Inverse $ X^{\\dagger} = \\Big( X^T X \\Big)^{-1} X^T $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Case : Invertible $ X^T X $, 唯一組解\n",
    "- Case : Singular $ X^T X $, 可能多組解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Algorithm\n",
    "\n",
    "#### STEP 1: From D, construct input matrix X and output vector y.\n",
    "\n",
    "X size: $ N \\times d+1 $  \n",
    "y length: N\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "\\vec{x}_1^T \\\\ \\vec{x}_2^T \\\\ \\vdots \\\\ \\vec{x}_N^T\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = \\begin{bmatrix}\n",
    "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2: Calculate pseudo-inverse matrix of X\n",
    "\n",
    "$$\n",
    "X^{\\dagger} = \\Big( X^T X \\Big)^{-1} X^T\n",
    "$$\n",
    "\n",
    "#### STEP 3:  return w\n",
    "\n",
    "$$\n",
    "w_{LIN} = X^{\\dagger} y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在得到 最佳解 $ w_{LIN} $ 後，即可進行預測 $ \\hat{y} = w_{LIN} x_n $\n",
    "\n",
    "代入上面的公式，也可知道 matrix formula of $ \\hat{y} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{bmatrix}\n",
    "\\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_N\n",
    "\\end{bmatrix}\n",
    " = X X^{\\dagger} y \n",
    "$\n",
    "\n",
    "$ X^{\\dagger}, y $ 是 in sample data\n",
    "\n",
    "$ X $ 是 $ x_n $ 組合的 prediction input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simpler-than-VC Guarantee\n",
    "\n",
    "TO SHOW: average of $ E_{in} = \\mathcal{E}_{D \\sim P^N} \\Big \\{ E_{in} (w_{LIN} \\text{ w.r.t. } D \\Big \\} = \\text{noise level} \\times \\Big( 1 - \\frac{d+1}{N} \\Big) $\n",
    "\n",
    "d+1: 自由度，有多少的 w  \n",
    "N: in sample 資料量  \n",
    "$ \\hat{y} $: predictions  \n",
    "call \"$ X X^\\dagger $\" : the \"hat matrix H\" because it puts ^ on y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "E_{in}(w_{LIN}) = \\frac{1}{N}\n",
    "\\begin{Vmatrix}\n",
    "y - \\hat{y}\n",
    "\\end{Vmatrix}^2\n",
    "$\n",
    "\n",
    "$ = \\frac{1}{N} \\begin{Vmatrix} y - X X^\\dagger y \\end{Vmatrix}^2 $\n",
    "\n",
    "$ = \\frac{1}{N} \\begin{Vmatrix} \\big( I - X X^\\dagger \\big) y \\end{Vmatrix}^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 $ \\Re^N $ 維度的空間中，  \n",
    "y = X w : 是任何一組解，也就是將 X Columns 做任意線性組合的結果, Span X  \n",
    "$ \\hat{y} = X w_{LIN} $ 是 y 落在 Span X 上的最佳解  \n",
    "向量: $ (y - \\hat{y}) $ 使得 $ \\hat{y} $ 最小 : $ (y - \\hat{y}) \\perp $ Span of X  \n",
    "hat matrix H:  project y to $ \\hat{y} \\in $ Span of X  \n",
    "I - H: transform y to $ (y - \\hat{y}) \\perp $ Span of X  \n",
    "\n",
    "claim: trace(I - H) = N - (d + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "average of $ E_{in} = \\text{ noise level } \\times \\big( 1 - \\frac{d+1}{N} \\big) $\n",
    "\n",
    "average of $ E_{out} = \\text{ noise level } \\times \\big( 1 + \\frac{d+1}{N} \\big) $\n",
    "\n",
    "both converge to $ \\delta^2 $ (noise level) for $ N \\to \\infty $\n",
    "\n",
    "expected generalization error: $ \\frac{2(d+1)}{N} $  \n",
    "similar to worst-case guarantee from VC.\n",
    "\n",
    "![img](./imgs/c09-learningcurves.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
