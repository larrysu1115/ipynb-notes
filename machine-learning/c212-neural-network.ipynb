{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "![img](imgs/c212-neural-network.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Layer to Multi-Layer\n",
    "\n",
    "單一層可以做到 OR / AND 的運算判別，但無法進行 XOR 的運算。\n",
    "\n",
    "使用雙層就可以做到 XOR。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tansformation function\n",
    "\n",
    "<img src=\"imgs/c212-tanh.png\" style=\"float:right\" />\n",
    "\n",
    "透過每一個 tanh 函數 (也可能是別的) 做階梯狀的切割\n",
    "\n",
    "$$\n",
    "\\tanh(s) = \\frac{exp(s) - exp(-s)}{exp(s) + exp(-s)} = 2 \\theta \\big( 2 s \\big) - 1\n",
    "$$\n",
    "\n",
    "### notations\n",
    "\n",
    "試想有 L 層的 NNet: \n",
    "\n",
    "$ d^{(0)} $ 是第零層的節點數  \n",
    "輸入的原資料: $ x_0=1, x_1, x_2, \\cdots, x_d $\n",
    "\n",
    "$\n",
    "d^{(0)} \\to d^{(1)} \\to d^{(2)} \\to \\cdots \\to d^{(L)}\n",
    "$\n",
    "\n",
    "進入下一層的加權 w\n",
    "\n",
    "$$\n",
    "w_{ij}^{(\\mathscr{l})} : \n",
    "\\begin{cases}\n",
    "1 \\le \\mathscr{l} \\le L & : layers \\\\\n",
    "0 \\le i \\le d^{(\\mathscr{l}-1)} & : inputs \\\\\n",
    "1 \\le j \\le d^{(\\mathscr{l})} & : outputs\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "每層的分數計算 score: (進入轉換前)\n",
    "\n",
    "$$\n",
    "s_j^{(\\mathscr{l})} = \\sum_{i=0}^{d^{(\\mathscr{l}-1)}}  w_{ij}^{(\\mathscr{l})} x_i^{(\\mathscr{l}-1)}\n",
    "$$\n",
    "\n",
    "每層的 $ x_i^{(\\mathscr{l})} $ : (轉換後得到的下一輪 x)\n",
    "\n",
    "$$ \n",
    "x_i^{(\\mathscr{l})} = \n",
    "\\begin{cases}\n",
    "tanh(s_j^{(\\mathscr{l})}) : & if \\ \\mathscr{l} \\lt L \\\\\n",
    "s_j^{(\\mathscr{l})} : & if \\ \\mathscr{l} = L\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNet Example\n",
    "\n",
    "NNet 3 layers : 3 - 2 - 3\n",
    "\n",
    "產生第一層的輸出 $ x^{(1)}_0=+1,\\ \\  x^{(1)}_1, \\ \\  x^{(1)}_2 $\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_0 & x_1 & x_2 & x_3\n",
    "\\end{bmatrix} \\ \\ \n",
    "\\begin{bmatrix}\n",
    "w^{(1)}_{01} & w^{(1)}_{02} \\\\\n",
    "w^{(1)}_{11} & w^{(1)}_{12} \\\\\n",
    "w^{(1)}_{21} & w^{(1)}_{22} \\\\\n",
    "w^{(1)}_{31} & w^{(1)}_{32} \\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "s^{(1)}_1 & s^{(1)}_2\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "+1 & tanh(s^{(1)}_1) & tanh(s^{(1)}_2)\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "+1 & x^{(1)}_1 & x^{(1)}_2\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "產生第二層的輸出 $ +1, x^{(2)}_1, x^{(2)}_2, x^{(2)}_3 $\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_0 & x^{(1)}_1 & x^{(1)}_2\n",
    "\\end{bmatrix} \\ \\ \n",
    "\\begin{bmatrix}\n",
    "w^{(2)}_{01} & w^{(2)}_{02} & w^{(1)}_{03} \\\\\n",
    "w^{(2)}_{11} & w^{(2)}_{12} & w^{(1)}_{13} \\\\\n",
    "w^{(2)}_{21} & w^{(2)}_{22} & w^{(1)}_{23} \\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "s^{(2)}_1 & s^{(2)}_2 & s^{(2)}_3\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "+1 & tanh(s^{(2)}_1) & tanh(s^{(2)}_2) & tanh(s^{(2)}_3)\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "+1 & x^{(2)}_1 & x^{(2)}_2 & x^{(2)}_3\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "當兩個向量越相似(平行)，內積越大; $ \\vec{x} \\cdot \\vec{w} $ 越相似，得出的 s 大，tanh(s) 才作用輸出下一層的 x\n",
    "\n",
    "可以看作符合某種模式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GOAL: learn all $ \\big\\{ w_{ij}^{(\\mathscr{l})} \\big\\} $ to **minimize** $ E_{in} \\Big( \\big\\{ w_{ij}^{(\\mathscr{l})} \\big\\} \\Big) $\n",
    "\n",
    "如果是只有一層的 hidden layer, 就是 aggregation of perceptrons 可以用 gradient boosting.  \n",
    "到了多層時，不是那麼容易；可以令結果是 $ NNet(x_n) $, 目標是:\n",
    "\n",
    "最小化 $ e_n = \\big( y_n - NNet(x_n) \\big)^2 $ \n",
    "\n",
    "如果可以知道 $ w_{ij} $ 的變化對 $ e_n $ 的影響, 也就是 : \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ e_n}{\\partial \\  w_{ij}^{(\\mathscr{l})} } \n",
    "$$\n",
    "\n",
    "就可以用 (Stochastic) Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先看最後一層 L\n",
    "\n",
    "先看最後一層的 $ \\frac{\\partial \\ e_n}{\\partial \\  w_{i1}^{(L)} } $ 是什麼。  \n",
    "$ j = 1 $, 因為是最後一層。  \n",
    "NNet(x) 的輸出其實就是最後一層唯一一個神經元的分數 s,  \n",
    "這個分數使用前一層的 w, x 來 $ \\sum $ 算出的\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "e_n = & \\big( y_n - & NNet(x_n) \\big)^2 \\\\\n",
    "    = & \\big( y_n - & s_1^{(L)} \\big)^2 \\\\\n",
    "    = & \\Big( y_n - & \\sum_{i=0}^{d^{(L-1)}} w_{i1}^{(L)} x_i^{(L-1)} \\Big)^2 \\\\    \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "據此來求得 e 對 w 的偏微分，可以先利用連鎖律 chain rule, 然後分別求 e對s, s對w 的偏微分結果，如下:\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial \\ e_n}{\\partial \\  w_{i1}^{(L)} } \\\\\n",
    "= \\frac{\\partial \\ e_n}{\\partial \\  s_1^{(L)}} \\times \\frac{\\partial \\ s_1^{(L)}}{\\partial \\  w_{i1}^{(L)}} \\\\\n",
    "= -2 \\big( y_n - s_1^{(L)} \\big) \\cdot \\big( x_i^{(L-1)} \\big)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 延伸到每一層\n",
    "\n",
    "延伸到每一層  \n",
    "$ 1 \\le \\mathscr{l} \\lt L$  \n",
    "$ 0 \\le i \\le d^{(\\mathscr{l}-1)} $  \n",
    "$ 1 \\le j \\le d^{(\\mathscr{l}-1)} $\n",
    "\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial \\ e_n}{\\partial \\  w_{ij}^{(\\mathscr{l})} } \\\\\n",
    "= \\frac{\\partial \\ e_n}{\\partial \\  s_j^{(\\mathscr{l})}} \\times \\frac{\\partial \\ s_j^{(\\mathscr{l})}}{\\partial \\  w_{ij}^{(\\mathscr{l})}} \\\\\n",
    "= \\delta_j^{(\\mathscr{l})} \\cdot \\big( x_i^{(\\mathscr{l}-1)} \\big)\n",
    "$$\n",
    "\n",
    "令 $ \\delta_j^{(\\mathscr{l})} = \\frac{\\partial \\ e_n}{\\partial \\  s_j^{(\\mathscr{l})}} $ : 錯誤 e 對於第 $ \\mathscr{l} $ 層分數 s 的變化量，最後一層 L 的例子上:\n",
    "\n",
    "$$\n",
    "\\delta_1^{(L)} = -2 \\big( y_n - s_1^{(L)} \\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "![img](imgs/c212-backprop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation (Backprop) Algorithm\n",
    "\n",
    "Backprop on NNet\n",
    "\n",
    "initial all weights $ w_{ij}^{(\\mathscr{l})} $,  \n",
    "for t = 0, 1, ..., T\n",
    "\n",
    "STEP 1 : Stochastic: randomly pick $ n \\in \\{ 1, 2, \\cdots, N \\} $\n",
    "\n",
    "STEP 2 : Forward: compute all $ x_i^{(\\mathscr{l})} $ with $ x^{(0)} = x_n $\n",
    "\n",
    "STEP 3 : Backward: compute all $ \\delta_j^{(\\mathscr{l})} subject \\ \\ to \\ \\ x^{(0)} = x_n $\n",
    "\n",
    "STEP 4 : Gradient Descent: $ w_{ij}^{(\\mathscr{l})} \\leftarrow w_{ij}^{(\\mathscr{l})} - \\eta \\ x_i^{(\\mathscr{l} - 1)} \\ \\delta_j^{(\\mathscr{l})} $\n",
    "\n",
    "return \n",
    "\n",
    "$$\n",
    "g_{NNET} \\big(x\\big) = \n",
    "\\Big(\n",
    "\\cdots \\ \\ \n",
    "tanh \\big(\n",
    "\\sum_j w_{jk}^{(2)} \\cdot tanh ( \\sum_i w_{ij}^{(1)} x_i )\n",
    "\\big)\n",
    "\\Big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch\n",
    "\n",
    "Step 1 ~ 3 is done parallelly many times and averate $ \\big( x_i^{(\\mathscr{l} - 1)} \\ \\delta_j^{(\\mathscr{l})} \\big) $ taken for update in Step 4, called mini-batch.\n",
    "\n",
    "Stochastic : 可以每次取多個，做平均梯度下降。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
