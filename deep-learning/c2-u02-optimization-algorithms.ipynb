{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization algorithms\n",
    "\n",
    "## Mini-batch gradient descent\n",
    "\n",
    "將資料分組如每 64, 128, 256, ... 個資料為一組，每一組記為 $ \\big( X^{\\{i\\}}, Y^{\\{i\\}} \\big) $\n",
    "\n",
    "如 m = 5,000,000 個資料，mini-batch size = 1000 每組一千個資料，\n",
    "\n",
    "- `X.shape = ( nx, m )`\n",
    "- `Y.shape = (  1, m )`\n",
    "- $ X^{\\{i\\}}$ `.shape = ( nx, 1000 ) `\n",
    "- $ Y^{\\{i\\}}$ `.shape = (  1, 1000 ) `\n",
    "- i = 1 ~ 5000\n",
    "\n",
    "One epoch of mini-batch GD:\n",
    "\n",
    "$\n",
    "\\text{for t = 1 ... 5000} \\\\\n",
    "\\ \\ \\ \\text{# forward prop on } X^{\\{t\\}} \\text{ for 1000 examples} \\\\\n",
    "\\ \\ \\ Z^{\\{t\\}} = W^{\\{t\\}} X^{\\{t\\}} + b^{\\{t\\}} \\\\\n",
    "\\ \\ \\ A^{\\{t\\}} = g^{\\{t\\}}\\big( Z^{\\{t\\}} \\big) \\\\\n",
    "\\ \\ \\ \\text{# Compute cost } \\\\\n",
    "\\ \\ \\ J^{\\{t\\}} = \\frac{1}{1000} \\sum_{i=1}^l \\mathcal{L} \\big( \\hat{y}^{(i)}, y^{(i)} \\big) +\n",
    "\\frac{\\lambda}{2 \\cdot 1000} \\sum_l \\Vert W^{\\{t\\}} \\Vert_F^2 \\\\\n",
    "\\ \\ \\ \\text{# back prop on } J^{\\{t\\}} \\\\\n",
    "\\ \\ \\ W^{[l]} = W^{[l]} - \\alpha \\cdot dW^{[l]} \\\\\n",
    "\\ \\ \\ b^{[l]} = b^{[l]} - \\alpha \\cdot db^{[l]} \\\\\n",
    "$\n",
    "\n",
    "在 Batch GD 的一個 epoch 只能做 一次 GD，用了 mini-batch 可以做 5000 次 GD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponentially Weighted (moving) Averages\n",
    "\n",
    "每組資料 $ (\\theta_1, \\theta_2, \\theta_3, \\dots ) $ 取前 約10組的平均:\n",
    "\n",
    "$\n",
    "v_0 = 0 \\\\\n",
    "v_1 = 0.9 v_0 + 0.1 \\theta_1 \\\\\n",
    "v_2 = 0.9 v_0 + 0.1 \\theta_2 \\\\\n",
    "\\vdots \\\\\n",
    "v_t = 0.9 v_{t-1} + 0.1 \\theta_t\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v_t = \\beta v_{t-1} + \\big( 1 - \\beta \\big) \\theta_t \\\\\n",
    "$$\n",
    "\n",
    "如果 $ \\theta_t $ 是每天氣溫，$ v_t $ 大約是前 $ \\frac{1}{1-\\beta} $ 天的平均氣溫。\n",
    "\n",
    "$ \\beta = 0.9 $, 就大約是前 10 天的平均。  \n",
    "$ \\beta = 0.98 $, 就大約是前 50 天的平均。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias correction\n",
    "\n",
    "在剛啟動時，前面幾筆會發生均值過低的問題，可以用 Bias correction 修正:\n",
    "\n",
    "$$\n",
    "v_t = \\frac{\\beta v_{t-1} + \\big( 1 - \\beta \\big) \\theta_t}{1 - \\beta^t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GD with Momentum\n",
    "\n",
    "在進行 mini-batch 時候，GD下降可能會偏移最佳方向，不停地震盪。希望能夠在 \"正確\"的方向維度上放大進度，\"錯誤\"的方向維度上縮小進度。\n",
    "\n",
    "這裏用 w 代表\"正確\"的方向維度，b 代表\"錯誤\"的方向維度。\n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "- $ \\alpha $ : learning rate\n",
    "- $ \\beta_1 $ : controller of Exponentially Weighted Average, 通常為 0.9\n",
    "\n",
    "On iteration t, compute dw, db on current mini-batch\n",
    "\n",
    "$$\n",
    "v_{dW} = \\beta_1 \\ v_{dW} + \\big( 1 - \\beta_1 \\big) dW \\\\\n",
    "v_{db} = \\beta_1 \\ v_{db} + \\big( 1 - \\beta_1 \\big) db \\\\\n",
    "W = W - \\alpha \\ v_{dW} \\\\\n",
    "b = b - \\alpha \\ v_{db}\n",
    "$$\n",
    "\n",
    "通常無需做 bias correction, 因為在 10 個 iteration 後幾乎已經沒影響。\n",
    "\n",
    "另一種版本: $ v_{dW} = \\beta \\ v_{dW} + dW $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop (Root Mean Square)\n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "- $ \\beta_2 $ : controller of Exponentially Weighted Average in RMSprop\n",
    "- $ \\epsilon = 10^{-8} $ : 避免除數為零。\n",
    "\n",
    "$$\n",
    "S_{dW} = \\beta_2 \\ S_{dW} + \\big( 1 - \\beta_2 \\big) dW^2 \\\\\n",
    "S_{db} = \\beta_2 \\ S_{db} + \\big( 1 - \\beta_2 \\big) db^2 \\\\\n",
    "W := W - \\alpha \\ \\frac{dw}{\\sqrt{S_{dW}} + \\epsilon} \\\\\n",
    "b := b - \\alpha \\ \\frac{db}{\\sqrt{S_{db}} + \\epsilon}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam optimization\n",
    "\n",
    "ADAptive Moment estimation\n",
    "\n",
    "Momentum + RMSprop, 要加上 bias correction\n",
    "\n",
    "$\n",
    "\\beta_1 = 0.9 \\to \\ \\ dW \\text{... (moment)} \\\\\n",
    "\\beta_2 = 0.999 \\to \\ \\ dW^2 \\text{... (second moment)} \\\\\n",
    "\\epsilon = 10^{-8}\n",
    "$\n",
    "\n",
    "update parameters using both RMSprop + Momentum\n",
    "\n",
    "$$\n",
    "v_{W^{[l]}} = \\beta_1 v_{W^{[l]}} + (1 - \\beta_1) \\frac{\\partial J }{ \\partial W^{[l]} } \\\\\n",
    "v^{corrected}_{W^{[l]}} = \\frac{v_{W^{[l]}}}{1 - (\\beta_1)^t} \\\\\n",
    "s_{W^{[l]}} = \\beta_2 s_{W^{[l]}} + (1 - \\beta_2) (\\frac{\\partial J }{\\partial W^{[l]} })^2 \\\\\n",
    "s^{corrected}_{W^{[l]}} = \\frac{s_{W^{[l]}}}{1 - (\\beta_2)^t} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{W^{[l]}}}{\\sqrt{s^{corrected}_{W^{[l]}}}+\\varepsilon}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Decay\n",
    "\n",
    "hyperparameters:\n",
    "\n",
    "- $ \\alpha_0 $\n",
    "- decay_rate\n",
    "\n",
    "$$\n",
    "\\alpha = \\frac{1}{1 + \\text{ decay_rate * epoch}} \\alpha_0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if $ \\alpha_0 = 0.2 $, decay_rate=1\n",
    "\n",
    "\n",
    "|epoch|alpha|\n",
    "|-|-|\n",
    "| 1 | 0.1 |\n",
    "| 2 | 0.067 |\n",
    "| 3 | 0.05 |\n",
    "| 4 | 0.04 |\n",
    "| 5 | 0.033 |\n",
    "| 6 | 0.029 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other learning rate decay methods:\n",
    "\n",
    "$\n",
    "\\alpha = 0.95^{\\text{epoch}} \\ \\alpha_0 \\\\\n",
    "\\alpha = \\frac{k}{\\sqrt{\\text{epoch}}} \\ \\alpha_0 \\\\\n",
    "\\alpha = \\frac{k}{\\sqrt{\\text{t}}} \\ \\alpha_0 \\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Optima &amp; Saddle Point\n",
    "\n",
    "絕大部分的 slope=0 的點，不是 Local Optima, 而是 Saddle Point\n",
    "\n",
    "在低維度的空間中，每個維度都是 convex 會形成 Local Optima,  \n",
    "但在 高維度的空間中，如 $ n_x=20000 $，要每個維度都是 convex 的機率實在很低。因此部份維度是 convex, 只是 saddle point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program Skeleton\n",
    "\n",
    "```\n",
    "parameters = initialize_parameters()\n",
    "v          = initialize_velocity # for momentum\n",
    "v, s       = initialize_adam     # for adam ( momentum + RMSprop )\n",
    "\n",
    "for i in range(num_epochs)\n",
    "   minibatches = random_mini_batches(X, Y, mini_batch_size, random_seed)\n",
    "   \n",
    "   for minibatch in minibatches\n",
    "      a3, caches = forward_propagation(minibatch_X, parameters)\n",
    "      cost       = compute_cost(a3, minibatch_Y)\n",
    "      grads      = backward_propagation(minibatch_X, minibatch_Y, caches)\n",
    "      parameters = update_parameters(...)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3cv",
   "language": "python",
   "name": "p3cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
