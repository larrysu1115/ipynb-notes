{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP - Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding 將每個詞編碼，但不知道每個詞之間的相似度。\n",
    "\n",
    "Word embedding 將詞分組列出權重。\n",
    "\n",
    "```\n",
    "        Man   Woman King  Queen Apple Orange\n",
    "Gender  1.    -1.    .8    -.75   .0     .01  \n",
    "Royal    .1     .12  .9     .92   .01    .0\n",
    "Food     .02    .0   .001   .01   .8     .73\n",
    "```\n",
    "\n",
    "如 Man 在 one-hot encoding 中是 832th, 則 man 在直的 column 為 $ e_{832} = \\begin{bmatrix} 1. \\\\ .1 \\\\ .02 \\end{bmatrix} $ \n",
    "\n",
    "則可以回答這樣的詞義問題:\n",
    "\n",
    "- a cup of apple ______.\n",
    "- a cup of orange _______."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize word embeddings\n",
    "\n",
    "T-SNE algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用大量 un-labeled text 獲得的 word embedding 結果。可以 transfer learning 用在 named entity recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogies\n",
    "\n",
    "$$\n",
    "e_{man} - e_{woman} \\approx \\begin{bmatrix}\n",
    "-2 \\\\ 0 \\\\ 0 \\\\ 0\n",
    "\\end{bmatrix}, \\ \\ \\ \\ \\ \n",
    "e_{king} - e_{queen} \\approx \\begin{bmatrix}\n",
    "-2 \\\\ 0 \\\\ 0 \\\\ 0\n",
    "\\end{bmatrix}, \\ \\ \\ \\ \\ \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "e_{man} - e_{woman} \\approx e_{king} - e_{?}\n",
    "$$\n",
    "\n",
    "find word w, such \n",
    "\n",
    "$arg \\max sim\\big( e_w, \\ \\ \\ e_{king} - e_{man} + e_{woman} \\big) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "$$\n",
    "sim \\big( u, v \\big) = \\frac{u^T v}{\\Vert u \\Vert_2 \\ \\ \\Vert v \\Vert_2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Matrix\n",
    "\n",
    "$\n",
    "E : 300 \\times 10000 \\\\\n",
    "O_{6235} : 10000 \\times 1 \\\\\n",
    "e_{6235} = E \\ O_{6235}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Skip Gram Model:\n",
    "\n",
    "```\n",
    "                  x   --->   y\n",
    "content:c (\"orange\")  --->  target:t (\"juice\")\n",
    "```\n",
    "\n",
    "$\n",
    "O_c \\to E \\to e_c = E O_c \\to SOFTMAX \\to \\hat{y}\n",
    "$\n",
    "\n",
    "parameter associate with output t: $ \\theta_t $, change output \"t\" be label.\n",
    "\n",
    "$$\n",
    "p\\big( t \\ \\big| \\ c \\big) = \\frac{e^{\\theta_t^T e_c}}{\\sum_j^{10000} e^{\\theta_t^T e_c}} \\\\\n",
    "\\mathcal{L} \\big( \\hat{y}, y \\big) = - \\sum_{i=1}^{10000} y_i \\log{\\hat{y}_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "缺點:計算太慢\n",
    "\n",
    "Hierarchical Softmax\n",
    "\n",
    "the other model: CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "\n",
    "```\n",
    "context  |  word   |  target?\n",
    "orange   |  juice  |       1\n",
    "orange   |   king  |       0\n",
    "orange   |   book  |       0\n",
    "orange   |    the  |       0\n",
    "orange   |     of  |       0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P\\big( y=1 \\ \\| \\ c,t  \\big) = \\sigma \\big( \\theta_t^T e_c \\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "給定 c, 對每一個 t (10000) 求 y 的 運算太大，因此隨機取 k 個 t, 做 binary classification.\n",
    "\n",
    "Selecting negative samples, 使用 最常出現的詞，或 每個詞平均機會，都不好。使用介於兩者之間的選擇方式:\n",
    "\n",
    "$\n",
    "P \\big( w_i \\big) = \\frac{f(w_i)^{3/4}}{\\sum_j^{10000} f(w_j)^{3/4}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe word vectors\n",
    "\n",
    "Global Vectors for word representation\n",
    "\n",
    "$ X_{ij} $ be the # of times `i (t)` appears in the context of `j (c)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimize:\n",
    "\n",
    "$$\n",
    "\\sum_i^{10000} \\sum_j^{10000} \n",
    "\\mathcal{f} \\big( X_{ij} \\big)\n",
    "\\big( \\theta_i^T e_j + b_i - b_j' - \\log{X_{ij}} \\big)^2\n",
    "$$\n",
    "\n",
    "$ \\mathcal{f} \\big( X_{ij} \\big) $ 處理次數 0 以及太頻繁或太少出現次數問題，使用的權重。\n",
    "\n",
    "$ \\theta_i, e_j $ are symmetric, $ e_w^{(final)} = \\frac{\\theta_w + e_w}{2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classification\n",
    "\n",
    "將 word embedding 送入 RNN 獲得 y. \n",
    "\n",
    "利用 pre-trained word embedding, 則只需要普通數量的 training set 就可訓練。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debiasing word embeddings\n",
    "\n",
    "性別 / 種族偏見？\n",
    "\n",
    "SVU : Singular Value Decomposition, PCA\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Identify bias direction\n",
    "1. Neutralize\n",
    "1. Equalize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3lab",
   "language": "python",
   "name": "p3lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
