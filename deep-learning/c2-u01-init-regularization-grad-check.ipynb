{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias &amp; Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume human erroe about 0 %\n",
    "\n",
    "|a|b|c|d|e|\n",
    "|-|-|-|-|-|\n",
    "| Training Set Error | 1%            | 15%       | 15%                | 0.5%              |\n",
    "| Dev Set Error      | 11%           | 16%       | 30%                | 1%                |\n",
    "|                    | High Variance | High Bias | High Var. and Bias | Low Var. and Bias |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Recipe\n",
    "\n",
    "`High Bias`\n",
    "\n",
    "- Bigger network\n",
    "- Train longer\n",
    "- Other NN architecture\n",
    "\n",
    "`High Variance`\n",
    "\n",
    "- More data\n",
    "- Regularization\n",
    "- Other NN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "## Logistic regression\n",
    "\n",
    "$$\n",
    "\\min_{w,b} \\mathcal{J} \\big( w, b \\big) \\\\\n",
    "\\mathcal{J} \\big( w, b \\big) = \\frac{1}{m} \\sum_{i=1}^m \n",
    "\\mathcal{L} \\big( \\hat{y}^{(i)}, y^{(i)} \\big) +\n",
    "\\frac{\\lambda}{2m} \\Vert w \\Vert_2^2 \n",
    "+ \\underbrace{\\frac{\\lambda}{2m} b^2}_{\\text{omit this item}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization\n",
    "\n",
    "$$\n",
    "\\Vert w \\Vert^2_2 = \\sum_{j=1}^{n_x} w_j^2 = w^T \\ w\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization\n",
    "\n",
    "w will be sparse. (more zero elements)\n",
    "\n",
    "$$\n",
    "\\frac{\\lambda}{2m} \\sum_{i=1}^{n_x} \\big| \\ w \\ \\big| = \n",
    "\\frac{\\lambda}{2m} \\Vert w \\Vert_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "$$\n",
    "\\mathcal{J} \\big( w^{[1]}, b^{[1]}, \\dots, w^{[L]}, b^{[L]} \\big) =\n",
    "\\frac{1}{m} \\sum_{i=1}^m \\mathcal{L} \\big( \\hat{y}^{(i)}, y^{(i)} \\big) +\n",
    "\\frac{\\lambda}{2m} \\sum_{l=1}^L \\Vert w^{[l]} \\Vert^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Frobenius Norm\" or \"Euclidean Norm\"\n",
    "\n",
    "$$\n",
    "\\Vert w^{[l]} \\Vert^2_F = \\sum_{i=1}^{n^{[l-1]}} \\ \\sum_{j=1}^{n^{[l]}} \n",
    "\\Big( w_{ij}^{[l]} \\Big)^2 \\\\\n",
    "w.shape = \\big( n^{[l]}, n^{[l-1]} \\big)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加上 Regularization term 後，Gradient Descent 變為:\n",
    "\n",
    "$$\n",
    "dw^{[l]} = \\big( \\text{ from back-propagation } \\big) + \\frac{\\lambda}{m} w^{[l]} \\\\\n",
    "\\begin{align}\n",
    "w^{[l]} & := w^{[l]} - \\alpha \\ dw^{[l]} \\\\\n",
    "        & = w^{[l]} - \\alpha \n",
    "\\big[ \\big( \\text{ from back-propagation } \\big) + \\frac{\\lambda}{m} w^{[l]} \\big] \\\\\n",
    "        & = w^{[l]} - \\frac{\\alpha \\lambda}{m} w^{[l]} -\n",
    "\\alpha \\big( \\text{ from back-propagation } \\big)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "會使得 $ w^{[l]} $ 更小一些，因此也叫做 weight decay 權重衰減"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "keep_prob = 0.8 用一個隨機矩陣，80% 的 w element 會保留\n",
    "\n",
    "``\n",
    "d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep_prob\n",
    "a3 = np.multiply(a3, d3)\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Regularization methods\n",
    "\n",
    "- Data augmentation: 將資料稍作變化，成為新 tagged data. 如將圖像水平翻轉。\n",
    "- Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up problem\n",
    "\n",
    "## Normalizing inputs\n",
    "\n",
    "Subtract mean\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{m} \\sum_{i=1}^m x^{(i)} \\\\\n",
    "x := x - \\mu\n",
    "$$\n",
    "\n",
    "Normalize Variance\n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\frac{1}{m} \\sum_{i=1}^m x^{(i)} ** 2 \\\\\n",
    "x := x / \\sigma^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing / Exploding gradients\n",
    "\n",
    "若是 初始權重 或 啟動函數 的值較大或較小，在深度的層進行 multiply 後，會變成 指數地巨大或微小，造成學習不易。需要謹慎地選擇初始權重。\n",
    "\n",
    "$$\n",
    "Var \\big( w_i \\big) = \\frac{1}{n} \\\\\n",
    "w^{[l]} = \\text{ np.random.rand(shape) * np.sqrt} \\big( \\frac{1}{n^{[l-1]}} \\big)\n",
    "$$\n",
    "\n",
    "若是使用 ReLU, 則變異數 VAR 設為 $ \\frac{2}{n} $\n",
    "\n",
    "若是使用 tanh, 則變異數 VAR 設為 $ \\sqrt{\\frac{1}{n^{[l-1]}}} $, 又叫做 `Xavier Initialization`\n",
    "\n",
    "另一個 Yoshua Bengio... 的版本是 VAR 設為 $ \\sqrt{\\frac{2}{n^{[l-1]} + n^{[l]}}} $\n",
    "\n",
    "paper (he initialization) : He et al. 2015, 變異數 VAR 設為 $ \\sqrt{\\frac{2}{n^{[l-1]}}} $\n",
    "\n",
    "``\n",
    "w_init = np.random.randn(layers_dim[l], layers_dim[l-1]) * np.sqrt( 2 / layers_dim[l-1] )\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Check\n",
    "\n",
    "Numerical approximation\n",
    "\n",
    "$$\n",
    "\\frac{f(\\theta+\\epsilon) \\ - \\ f(\\theta-\\epsilon)}{2 \\epsilon} \\approx g\\big( \\theta \\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將 $ w^{[1]}, b^{[1]}, \\dots, w^{[L]}, b^{[L]} $ reshape 成一個長 vector $ \\theta $\n",
    "\n",
    "將 $ dw^{[1]}, db^{[1]}, \\dots, dw^{[L]}, db^{[L]} $ reshape 成一個長 vector $ d\\theta $\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{for each i: } & \\\\\n",
    "                    & d\\theta_{\\text{approx}}[i] = \n",
    "\\frac{\n",
    "\\mathcal{J}\\big( \\theta_1, \\theta_2, \\dots, \\theta_i + \\epsilon, \\dots \\big) -\n",
    "\\mathcal{J}\\big( \\theta_1, \\theta_2, \\dots, \\theta_i - \\epsilon, \\dots \\big)}\n",
    "{2 \\epsilon} \\\\\n",
    "& d\\theta[i] = \\frac{\\partial \\mathcal{J}}{\\partial{\\theta_i}} \\\\\n",
    "& d\\theta_{\\text{approx}}[i] \\approx d\\theta[i] \\\\\n",
    "& \\text{ Check: }\n",
    "\\frac{\\Vert d\\theta_{\\text{approx}} - d\\theta \\Vert_2}\n",
    "{\\Vert d\\theta_{\\text{approx}} \\Vert_2 + \\Vert d\\theta \\Vert_2}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\epsilon = 10^{-7} $\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\lt 10^{-7} : & \\text{ good } \\\\\n",
    "\\approx 10^{-5} : & \\text{ maybe okay } \\\\\n",
    "\\gt 10^{-3} : & \\text{ worry } \\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Don't use \"grad check\" in training, only for debug.\n",
    "- \"grad check\" doesn't work with dropout."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3cv",
   "language": "python",
   "name": "p3cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
