{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Given $ x \\in \\mathcal{R}^{n_x} $,  \n",
    "want $ \\hat{y} = P \\big( y=1 \\ \\Big| \\ x \\big) , \\ 0 \\le \\hat{y} \\le 1 $  \n",
    "Parameters : $ w \\in \\mathcal{R}^{n_x},\\ b \\in \\mathcal{R} $\n",
    "\n",
    "Output: $$ \\hat{y} = \\sigma \\Big( w^T x + b \\Big) $$\n",
    "\n",
    "Sigmoid function: $ \\sigma \\big( z \\big) = \\frac{1}{1+e^{-z}} $,\n",
    "若 z 非常大，sigma 趋近 1; 若 z 非常小, sigma 趋近 0，\n",
    "\n",
    "使用 Logistic Regression 是要学习出 w, b；使得 $ \\hat{y} $ 为 : [ y等于1的几率 ]\n",
    "\n",
    "### Lost Function : L\n",
    "\n",
    "为了学习到好的 w, b，需要定义 Lost Function : $ \\mathcal{L} $\n",
    "\n",
    "$$\n",
    "\\mathcal{L} \\big( \\hat{y}, y \\big) =\n",
    "- \\Big( y \\log\\hat{y} + (1-y) \\log\\big( 1 - \\hat{y} \\big) \\Big)\n",
    "$$\n",
    "\n",
    "不使用平方差，因为平方差在这里无法获得 convex function 凸函数。\n",
    "\n",
    "### Intuition: \n",
    "\n",
    "若 y=1, $ \\mathcal{L} = - \\Big( \\log\\hat{y} \\Big) $, 要L最小，就是希望 $ \\log\\hat{y} $ 越大越好，就是希望 $ \\hat{y} $ 最大，而在 sigmoid function 限制下 y 最大值不超过1，就是希望 $ \\hat{y} $ 尽量接近 1。\n",
    "\n",
    "若 y=0, $ \\mathcal{L} = - \\Big( \\log \\big( 1 - \\hat{y} \\big) \\Big) $，要L最小，就是希望 $ \\log(1-\\hat{y}) $ 越大越好，就是希望 $ ( 1 - \\hat{y} ) $ 最大，而在 sigmoid function 限制下 y 最小值不低于 0，就是希望 $ \\hat{y} $ 尽量接近0。\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{IF } y = 1: & \\ p\\big( y \\ \\big| \\ x \\big) = \\hat{y} \\\\\n",
    "\\text{IF } y = 0: & \\ p\\big( y \\ \\big| \\ x \\big) = 1 - \\hat{y}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "可以将上面 binary 的两种可能汇总成下面的式子，下式带入 y=0, y=1 都可得到上式：\n",
    "\n",
    "$$\n",
    "p\\big( y \\ \\big| \\ x \\big) = \\hat{y}^y \\big( 1 - \\hat{y} \\big)^{1-y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "想要的结果是最大化，将上式取 log 后依然是要最大化:\n",
    "\n",
    "$$\n",
    "\\log p\\big( y \\ \\big| \\ x \\big) = y \\log \\hat{y} + (1-y) \\log \\big( 1 - \\hat{y} \\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function : J\n",
    "\n",
    "Lost function L 是对单一的 training example : $ x^{(i)} $ 做定义。引伸到整个 training dataset, 定义一个 Cost Function : J\n",
    "\n",
    "$$\n",
    "\\mathcal{J} \\big( w, b \\big) = \n",
    "\\frac{1}{m} \n",
    "\\sum_{i=1}^m \\mathcal{L} \\big( \\hat{y}, y \\big) =\n",
    "- \\frac{1}{m} \n",
    "\\sum_{i=1}^m \\Big[ \n",
    "y^{(i)} \\log\\hat{y}^{(i)} + \n",
    "\\big( 1 - y^{(i)} \\big) \\log \\big( 1 - \\hat{y}^{(i)} \\big)\n",
    "\\Big]\n",
    "$$\n",
    "\n",
    "m: number of training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p\\big( \\text{ labels in training set } \\big) & = \\log \\prod_{i=1}^m p \\big( y^{(i)} \\ \\big| \\ x^{(i)} \\big) \\\\\n",
    "& = \\sum_{i=1}^m \\log p \\big( y^{(i)} \\ \\big| \\ x^{(i)} \\big) \n",
    "\\text{ ...using Maximum Likelihood Estimation} \\\\\n",
    "& = - \\sum_{i=1}^m \\mathcal{L} \\big( \\hat{y}^{(i)}, y^{(i)} \\big) \\\\\n",
    "\\text{Cost } J \\big( w, b \\big) & = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L} \\big( \\hat{y}^{(i)}, y^{(i)} \\big)\n",
    "\\end{align} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "简化 $ \\mathcal{J}(w,b) $ 为 $ \\mathcal{J}(w) $,  \n",
    "在多维度的空间上，  \n",
    "y 为 J(w)  \n",
    "$ x_1, x_2, x_3, \\dots $ 为 w\n",
    "\n",
    "由 multi-variables Calculus 理论可知，将 $ w - \\alpha \\frac{d\\ J(w)}{d\\ w} $ 会逐步接近最低点的 J(w)  \n",
    "此处 $ \\alpha $ 为 `learning rate`,  \n",
    "将 $ \\frac{d\\ J(w)}{d\\ w} $ 在程序中标记为 `dw`, 则下面的 pseudo-code 表示了 gradient descent 获得最佳解的过程:\n",
    "\n",
    "```code\n",
    "Repeat {\n",
    "  w := w - alpha * dw\n",
    "}\n",
    "```\n",
    "\n",
    "以 $ J(w,b) $ 来看，就是 \n",
    "$$\n",
    "w := w - \\alpha \\times \\frac{\\partial \\ J(w, b)}{\\partial w} \\\\\n",
    "b := b - \\alpha \\times \\frac{\\partial \\ J(w, b)}{\\partial b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Computation Graph\n",
    "- Derivatives\n",
    "- Chain Rule\n",
    "\n",
    "$\n",
    "a, b, c \\\\\n",
    "u = b \\times c \\\\\n",
    "v = a + u \\\\\n",
    "J = 3v\n",
    "$\n",
    "\n",
    "Chain Rule:\n",
    "\n",
    "$$\n",
    "\\frac{d\\ J}{d a} = \\frac{d\\ J}{d v} \\ \\frac{d\\ v}{d a}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation Graph\n",
    "\n",
    "an example with 2 features: $ x_1, x_2 $\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\ w_1 \\\\ x_2 \\\\ w_2 \\\\ b\n",
    "\\end{bmatrix} \\Rightarrow \n",
    "\\begin{bmatrix}\n",
    "z = w_1 x_1 + w_2 x_2 + b\n",
    "\\end{bmatrix} \\Rightarrow \n",
    "\\begin{bmatrix}\n",
    "a = \\sigma\\big( z \\big)\n",
    "\\end{bmatrix} \\Rightarrow\n",
    "\\begin{bmatrix}\n",
    "\\mathcal{L} \\big( a, y \\big)\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从最右边的 Lost Function: L 利用导数推回，可得每一步骤变量(z,a) 对最后损失函数的影响。\n",
    "\n",
    "从 L 推回 a: \"da\"\n",
    "\n",
    "$$\n",
    "\\text{\"da\"} = \n",
    "\\frac{d}{da} \\mathcal{L} \\big( a,y \\big) = \n",
    "- \\frac{y}{a} + \\frac{1-y}{1-a}\n",
    "$$\n",
    "\n",
    "从 L 推回 z: \"dz\"\n",
    "\n",
    "$$\n",
    "\\text{\"dz\"} = \n",
    "\\frac{d}{dz} \\mathcal{L}(a,y) = a - y \\\\\n",
    "= \\frac{d}{dz} a \\times \\frac{d}{da} \\mathcal{L}(a,y) \\\\\n",
    "= \\frac{d}{dz} \\sigma(z) \\times \\frac{d}{da} \\mathcal{L}(a,y) \\\\\n",
    "= \\Big( a(1-a) \\Big) \\times \\Big( - \\frac{y}{a} + \\frac{1-y}{1-a} \\Big) = a - y\n",
    "$$\n",
    "\n",
    "从 L 推回 w1: \"dw1\"\n",
    "\n",
    "$$\n",
    "\\text{\"dw1\"} = \\frac{d}{d w_1} \\mathcal{L}(a,y) \\\\\n",
    "= \\frac{dz}{d w_1} \\times \\frac{d}{dz} \\mathcal{L}(a,y) \\\\\n",
    "= \\frac{d}{d w_1} \\big( w_1 x_1 + w_2 x_2 + b \\big) \\ \\times \\ (a-y) \\\\\n",
    "= w_1 \\times (a-y)\n",
    "$$\n",
    "\n",
    "从 L 推回 b: \"db\"\n",
    "\n",
    "$$\n",
    "\\text{\"db\"} = (a-y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and Backward propagation\n",
    "\n",
    "Forward Propagation:\n",
    "- You get X\n",
    "- You compute $A = \\sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$\n",
    "\n",
    "gradient descent: $ \\theta = \\theta - \\alpha \\text{ } d\\theta$\n",
    "\n",
    "prediction: $\\hat{Y} = A = \\sigma(w^T X + b)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3cv",
   "language": "python",
   "name": "p3cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
